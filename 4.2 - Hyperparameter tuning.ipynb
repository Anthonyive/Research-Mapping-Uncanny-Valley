{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "%rm -rf ./tensorboard_logs/\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepy = pd.read_pickle('./pickles/creepy.pickle')\n",
    "noncreepy = pd.read_pickle('./pickles/noncreepy.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>sum_vec</th>\n",
       "      <th>log_(score+min+0.01)</th>\n",
       "      <th>sum_vec_with_log_prepended</th>\n",
       "      <th>selftext_sents_count</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gxsa0i</td>\n",
       "      <td>Do NOT Open Your Eyes... (Pt. 1)</td>\n",
       "      <td>This is the only rule of our household. If you...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.940986, -5.617336, 11.896912, -15.436118, -...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, 2.940986, -5.617336, 11.896912, -1...</td>\n",
       "      <td>38</td>\n",
       "      <td>[0.6981347, 0.077394366, -0.14782463, 0.313076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gxs6jf</td>\n",
       "      <td>Do NOT open your eyes. (The Beginning)</td>\n",
       "      <td>This is the only rule of our household. If you...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.940986, -5.617336, 11.896912, -15.436118, -...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, 2.940986, -5.617336, 11.896912, -1...</td>\n",
       "      <td>38</td>\n",
       "      <td>[0.6981347, 0.077394366, -0.14782463, 0.313076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gxrytp</td>\n",
       "      <td>My Best Friend Saw Bugs Under His Skin</td>\n",
       "      <td>It is hard for me to talk about my old friend ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-19.201273, -13.715499, 24.393753, -33.97739,...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -19.201273, -13.715499, 24.393753,...</td>\n",
       "      <td>81</td>\n",
       "      <td>[0.6981347, -0.23705275, -0.16932714, 0.301157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gxrnj7</td>\n",
       "      <td>I picked up a hitchhiker by mistake, now he's ...</td>\n",
       "      <td>They say the devil is in the details.  Well th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-60.77887, -33.726135, 119.47121, -95.021385,...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -60.77887, -33.726135, 119.47121, ...</td>\n",
       "      <td>429</td>\n",
       "      <td>[0.6981347, -0.14167568, -0.0786157, 0.2784876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gxrm8v</td>\n",
       "      <td>I'm tasked with killing nameless things out in...</td>\n",
       "      <td>“Any sign of ‘em yet?” \\n\\nI continued staring...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-22.785084, 14.806147, 26.129469, -24.832222,...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -22.785084, 14.806147, 26.129469, ...</td>\n",
       "      <td>177</td>\n",
       "      <td>[0.6981347, -0.12872928, 0.083650544, 0.147624...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15367</th>\n",
       "      <td>15367</td>\n",
       "      <td>eihp0m</td>\n",
       "      <td>Hylophobia</td>\n",
       "      <td>*There is no cure for trauma. Once it enters t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-51.169456, -5.836507, 70.7862, -111.05138, 7...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -51.169456, -5.836507, 70.7862, -1...</td>\n",
       "      <td>447</td>\n",
       "      <td>[0.6981347, -0.11447306, -0.013057062, 0.15835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15368</th>\n",
       "      <td>15368</td>\n",
       "      <td>eihmg7</td>\n",
       "      <td>I adopted my late sisters orphaned child. This...</td>\n",
       "      <td>I knew Persephone would need time to adjust, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-22.327501, -34.563572, 39.197514, -72.437645...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -22.327501, -34.563572, 39.197514,...</td>\n",
       "      <td>246</td>\n",
       "      <td>[0.6981347, -0.0907622, -0.14050232, 0.1593394...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15369</th>\n",
       "      <td>15369</td>\n",
       "      <td>eihgtp</td>\n",
       "      <td>My first paranormal experience!!</td>\n",
       "      <td>This isnt much, but this is surely the first u...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-4.7810946, 0.12821773, 2.941056, -3.949329, ...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -4.7810946, 0.12821773, 2.941056, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.6981347, -0.39842454, 0.010684811, 0.245088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15370</th>\n",
       "      <td>15370</td>\n",
       "      <td>eigzgj</td>\n",
       "      <td>I met the demon under my bed... Its not what I...</td>\n",
       "      <td>Okay. for context, this story started about a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.7662485, -4.471965, 8.116567, -12.409156, ...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, -1.7662485, -4.471965, 8.116567, -...</td>\n",
       "      <td>30</td>\n",
       "      <td>[0.6981347, -0.05887495, -0.1490655, 0.2705522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15371</th>\n",
       "      <td>15371</td>\n",
       "      <td>eiggxr</td>\n",
       "      <td>The Perfect Wife</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\nI  was never able to find love...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.2265594, -10.923081, 8.348171, -10.28161, 0...</td>\n",
       "      <td>0.698135</td>\n",
       "      <td>[0.6981347, 2.2265594, -10.923081, 8.348171, -...</td>\n",
       "      <td>49</td>\n",
       "      <td>[0.6981347, 0.04543999, -0.22292003, 0.1703708...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15372 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      id                                              title  \\\n",
       "0          0  gxsa0i                   Do NOT Open Your Eyes... (Pt. 1)   \n",
       "1          1  gxs6jf             Do NOT open your eyes. (The Beginning)   \n",
       "2          2  gxrytp             My Best Friend Saw Bugs Under His Skin   \n",
       "3          3  gxrnj7  I picked up a hitchhiker by mistake, now he's ...   \n",
       "4          4  gxrm8v  I'm tasked with killing nameless things out in...   \n",
       "...      ...     ...                                                ...   \n",
       "15367  15367  eihp0m                                         Hylophobia   \n",
       "15368  15368  eihmg7  I adopted my late sisters orphaned child. This...   \n",
       "15369  15369  eihgtp                   My first paranormal experience!!   \n",
       "15370  15370  eigzgj  I met the demon under my bed... Its not what I...   \n",
       "15371  15371  eiggxr                                   The Perfect Wife   \n",
       "\n",
       "                                                selftext  score  \\\n",
       "0      This is the only rule of our household. If you...      1   \n",
       "1      This is the only rule of our household. If you...      1   \n",
       "2      It is hard for me to talk about my old friend ...      1   \n",
       "3      They say the devil is in the details.  Well th...      1   \n",
       "4      “Any sign of ‘em yet?” \\n\\nI continued staring...      1   \n",
       "...                                                  ...    ...   \n",
       "15367  *There is no cure for trauma. Once it enters t...      1   \n",
       "15368  I knew Persephone would need time to adjust, b...      1   \n",
       "15369  This isnt much, but this is surely the first u...      1   \n",
       "15370  Okay. for context, this story started about a ...      1   \n",
       "15371  &amp;#x200B;\\n\\nI  was never able to find love...      1   \n",
       "\n",
       "                                                 sum_vec  \\\n",
       "0      [2.940986, -5.617336, 11.896912, -15.436118, -...   \n",
       "1      [2.940986, -5.617336, 11.896912, -15.436118, -...   \n",
       "2      [-19.201273, -13.715499, 24.393753, -33.97739,...   \n",
       "3      [-60.77887, -33.726135, 119.47121, -95.021385,...   \n",
       "4      [-22.785084, 14.806147, 26.129469, -24.832222,...   \n",
       "...                                                  ...   \n",
       "15367  [-51.169456, -5.836507, 70.7862, -111.05138, 7...   \n",
       "15368  [-22.327501, -34.563572, 39.197514, -72.437645...   \n",
       "15369  [-4.7810946, 0.12821773, 2.941056, -3.949329, ...   \n",
       "15370  [-1.7662485, -4.471965, 8.116567, -12.409156, ...   \n",
       "15371  [2.2265594, -10.923081, 8.348171, -10.28161, 0...   \n",
       "\n",
       "       log_(score+min+0.01)  \\\n",
       "0                  0.698135   \n",
       "1                  0.698135   \n",
       "2                  0.698135   \n",
       "3                  0.698135   \n",
       "4                  0.698135   \n",
       "...                     ...   \n",
       "15367              0.698135   \n",
       "15368              0.698135   \n",
       "15369              0.698135   \n",
       "15370              0.698135   \n",
       "15371              0.698135   \n",
       "\n",
       "                              sum_vec_with_log_prepended  \\\n",
       "0      [0.6981347, 2.940986, -5.617336, 11.896912, -1...   \n",
       "1      [0.6981347, 2.940986, -5.617336, 11.896912, -1...   \n",
       "2      [0.6981347, -19.201273, -13.715499, 24.393753,...   \n",
       "3      [0.6981347, -60.77887, -33.726135, 119.47121, ...   \n",
       "4      [0.6981347, -22.785084, 14.806147, 26.129469, ...   \n",
       "...                                                  ...   \n",
       "15367  [0.6981347, -51.169456, -5.836507, 70.7862, -1...   \n",
       "15368  [0.6981347, -22.327501, -34.563572, 39.197514,...   \n",
       "15369  [0.6981347, -4.7810946, 0.12821773, 2.941056, ...   \n",
       "15370  [0.6981347, -1.7662485, -4.471965, 8.116567, -...   \n",
       "15371  [0.6981347, 2.2265594, -10.923081, 8.348171, -...   \n",
       "\n",
       "       selftext_sents_count                                                vec  \n",
       "0                        38  [0.6981347, 0.077394366, -0.14782463, 0.313076...  \n",
       "1                        38  [0.6981347, 0.077394366, -0.14782463, 0.313076...  \n",
       "2                        81  [0.6981347, -0.23705275, -0.16932714, 0.301157...  \n",
       "3                       429  [0.6981347, -0.14167568, -0.0786157, 0.2784876...  \n",
       "4                       177  [0.6981347, -0.12872928, 0.083650544, 0.147624...  \n",
       "...                     ...                                                ...  \n",
       "15367                   447  [0.6981347, -0.11447306, -0.013057062, 0.15835...  \n",
       "15368                   246  [0.6981347, -0.0907622, -0.14050232, 0.1593394...  \n",
       "15369                    12  [0.6981347, -0.39842454, 0.010684811, 0.245088...  \n",
       "15370                    30  [0.6981347, -0.05887495, -0.1490655, 0.2705522...  \n",
       "15371                    49  [0.6981347, 0.04543999, -0.22292003, 0.1703708...  \n",
       "\n",
       "[15372 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepy_sum_vec_with_log_prepended = creepy.loc[:,'vec'].copy()\n",
    "noncreepy_sum_vec_with_log_prepended = noncreepy.loc[:,'vec'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15372, 769)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creepy_features = pd.DataFrame(creepy_sum_vec_with_log_prepended.to_list()).to_numpy(dtype = float)\n",
    "creepy_labels = np.ones(len(creepy_features))\n",
    "creepy_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22474, 769)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noncreepy_features = pd.DataFrame(noncreepy_sum_vec_with_log_prepended.to_list()).to_numpy(dtype = float)\n",
    "noncreepy_labels = np.zeros(len(noncreepy_features))\n",
    "noncreepy_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creepy_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69813472  0.07739437 -0.14782463 ... -0.12086978  0.00944329\n",
      "  -0.23866224]\n",
      " [ 0.69813472  0.07739437 -0.14782463 ... -0.12086978  0.00944329\n",
      "  -0.23866224]\n",
      " [ 0.69813472 -0.23705275 -0.16932714 ...  0.22444913 -0.20354605\n",
      "  -0.08335517]\n",
      " ...\n",
      " [ 0.69813472 -0.06071103  0.02235722 ...  0.10523307 -0.29174414\n",
      "  -0.14749481]\n",
      " [ 0.69813472  0.14904401 -0.10234425 ... -0.13638787  0.24349198\n",
      "   0.0390255 ]\n",
      " [ 0.69813472 -0.24383056 -0.36296186 ...  0.32824722 -0.14198837\n",
      "  -0.42668518]] [1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "features = np.concatenate((creepy_features, noncreepy_features))\n",
    "labels = np.concatenate((creepy_labels, noncreepy_labels))\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37846, 769) (37846,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69813472  0.05087078 -0.17453498 ... -0.10480397 -0.00878134\n",
      "  -0.02162955]\n",
      " [ 0.69813472  0.01022636 -0.02600729 ...  0.2258952  -0.11448984\n",
      "   0.0233674 ]\n",
      " [ 0.69813472  0.06874519 -0.01141187 ...  0.04425691 -0.2106467\n",
      "  -0.09333058]\n",
      " ...\n",
      " [ 0.69813472 -0.1532692  -0.02440793 ...  0.14036703  0.00953849\n",
      "   0.07714657]\n",
      " [ 1.10194004 -0.21753943 -0.20378701 ...  0.08343509  0.14993337\n",
      "   0.17935309]\n",
      " [ 0.69813472  0.29268453  0.09761263 ... -0.05415852 -0.49108347\n",
      "  -0.5703575 ]] [0. 1. 0. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "features, labels = shuffle(features, labels)\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.51793901 0.40268144 ... 0.47184547 0.52893419 0.43782805]\n",
      " [0.         0.5063239  0.45227568 ... 0.57464027 0.49058261 0.45035605]\n",
      " [0.         0.52304705 0.45714918 ... 0.51817968 0.45569642 0.41786513]\n",
      " ...\n",
      " [0.         0.45960116 0.45280972 ... 0.54805463 0.53558072 0.46532919]\n",
      " [0.04952086 0.44123442 0.39291401 ... 0.53035785 0.58651669 0.49378542]\n",
      " [0.         0.58704303 0.49355308 ... 0.48758815 0.35395254 0.2850518 ]] [0. 1. 0. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "print(scaled_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] 70% train, 15% val, 15% test\n",
    " - Train: 26500\n",
    " - Valid: 5677\n",
    " - Test: 5669\n",
    "- [ ] 80% train, 10% val, 10% test\n",
    "- [ ] 60% train, 20% val, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.        , 0.04952086,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = scaled_features[:26500], scaled_features[26500:26500+5677], scaled_features[26500+5677:]\n",
    "y_train, y_valid, y_test = labels[:26500], labels[26500:26500+5677], labels[26500+5677:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "#     keras.layers.Dense(units = 303, input_shape = (769,), activation = 'relu'),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units = 303, input_shape = (769,), activation = 'relu'),\n",
    "    keras.layers.Dropout(0.1),\n",
    "#     keras.layers.Dense(units = 256, activation = 'relu'),\n",
    "    keras.layers.Dense(units = 128, activation = 'relu'),\n",
    "    keras.layers.Dense(units = 64, activation = 'relu'),\n",
    "    keras.layers.Dense(units = 32, activation = 'relu'),\n",
    "    keras.layers.Dense(units = 16, activation = 'relu'),\n",
    "    keras.layers.Dense(units = 1, activation = 'sigmoid') # here the units must be 1 in order for binary classifications to work\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 303)               233310    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 303)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               38912     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 283,103\n",
      "Trainable params: 283,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = keras.optimizers.Adam(lr=0.000959, beta_1 = 0.9, beta_2=0.999), # you can tune the learning rate here. Default lr = 0.01\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.6924 - accuracy: 0.5000WARNING:tensorflow:From /Users/anthony/Documents/GitHub/Research-Mapping-Uncanny-Valley/.venv/creepyvenv/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0110s). Check your callbacks.\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.4054 - accuracy: 0.8001 - val_loss: 0.3031 - val_accuracy: 0.8723\n",
      "Epoch 2/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2622 - accuracy: 0.8973 - val_loss: 0.3511 - val_accuracy: 0.8755\n",
      "Epoch 3/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2640 - accuracy: 0.8944 - val_loss: 0.2416 - val_accuracy: 0.9010\n",
      "Epoch 4/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2841 - accuracy: 0.8798 - val_loss: 0.3652 - val_accuracy: 0.8445\n",
      "Epoch 5/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2672 - accuracy: 0.8833 - val_loss: 0.1900 - val_accuracy: 0.9292\n",
      "Epoch 6/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2714 - accuracy: 0.8809 - val_loss: 0.2171 - val_accuracy: 0.9225\n",
      "Epoch 7/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2587 - accuracy: 0.8879 - val_loss: 0.3863 - val_accuracy: 0.8351\n",
      "Epoch 8/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2578 - accuracy: 0.8888 - val_loss: 0.2007 - val_accuracy: 0.9301\n",
      "Epoch 9/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2451 - accuracy: 0.8944 - val_loss: 0.2206 - val_accuracy: 0.9267\n",
      "Epoch 10/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2569 - accuracy: 0.8879 - val_loss: 0.1700 - val_accuracy: 0.9394\n",
      "Epoch 11/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2358 - accuracy: 0.8962 - val_loss: 0.3281 - val_accuracy: 0.8364\n",
      "Epoch 12/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2477 - accuracy: 0.8938 - val_loss: 0.2237 - val_accuracy: 0.9049\n",
      "Epoch 13/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2425 - accuracy: 0.8927 - val_loss: 0.1879 - val_accuracy: 0.9341\n",
      "Epoch 14/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2359 - accuracy: 0.8986 - val_loss: 0.2430 - val_accuracy: 0.8973\n",
      "Epoch 15/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2401 - accuracy: 0.8932 - val_loss: 0.2356 - val_accuracy: 0.8968\n",
      "Epoch 16/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2337 - accuracy: 0.9000 - val_loss: 0.1636 - val_accuracy: 0.9422\n",
      "Epoch 17/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2277 - accuracy: 0.9023 - val_loss: 0.1681 - val_accuracy: 0.9399\n",
      "Epoch 18/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8996 - val_loss: 0.1683 - val_accuracy: 0.9373\n",
      "Epoch 19/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2262 - accuracy: 0.9008 - val_loss: 0.1712 - val_accuracy: 0.9454\n",
      "Epoch 20/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2295 - accuracy: 0.8995 - val_loss: 0.1580 - val_accuracy: 0.9450\n",
      "Epoch 21/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2241 - accuracy: 0.9022 - val_loss: 0.1522 - val_accuracy: 0.9435\n",
      "Epoch 22/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2232 - accuracy: 0.9020 - val_loss: 0.1585 - val_accuracy: 0.9438\n",
      "Epoch 23/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.2220 - accuracy: 0.9029 - val_loss: 0.1979 - val_accuracy: 0.9369\n",
      "Epoch 24/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2244 - accuracy: 0.9021 - val_loss: 0.1503 - val_accuracy: 0.9491\n",
      "Epoch 25/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2214 - accuracy: 0.9036 - val_loss: 0.1798 - val_accuracy: 0.9304\n",
      "Epoch 26/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2256 - accuracy: 0.9014 - val_loss: 0.1936 - val_accuracy: 0.9456\n",
      "Epoch 27/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2218 - accuracy: 0.9037 - val_loss: 0.1721 - val_accuracy: 0.9315\n",
      "Epoch 28/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2205 - accuracy: 0.9020 - val_loss: 0.1708 - val_accuracy: 0.9375\n",
      "Epoch 29/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2181 - accuracy: 0.9028 - val_loss: 0.1921 - val_accuracy: 0.9195\n",
      "Epoch 30/30\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.2159 - accuracy: 0.9055 - val_loss: 0.1573 - val_accuracy: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 52840), started 0:01:56 ago. (Use '!kill 52840' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dbbf0b4ec1e0fe64\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dbbf0b4ec1e0fe64\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "\n",
    "def get_run_log_dir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_log_dir()\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 30, \n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[tensorboard_cb])\n",
    "\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEzCAYAAAACSWsXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABeo0lEQVR4nO3dd5wURdrA8V9N2pzZvEsSAYmS0xEMKCpiRDCdoMDhnXqeOWfPnM5XUVQwC6igGFEUJEpGkZzZyOYcJtX7x8wuu7CRnd2B5fnqfLqnu6f7md5hnqmq7iqltUYIIYQQ3mPwdgBCCCHEqU6SsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF5WbzJWSs1SSmUopf6qZb1SSv1PKbVHKfWnUqqv58MUQgghWq+GlIzfB8bUsf4C4HT3Yxowo+lhCSGEEKeOepOx1noZkFPHJpcAH2qX34FQpVSspwIUQgghWjtPtBnHA0lVnie7lwkhhBCiAUwteTCl1DRcVdkEBAT069q1a0seXgghhPCaDRs2ZGmtI2ta54lknAIkVnme4F52DK31TGAmQP/+/fX69es9cHghhBDixKeUOljbOk9UUy8E/u6+qnowkK+1TvPAfoUQQohTQr0lY6XUZ8AooI1SKhl4FDADaK3fAr4HLgT2ACXA5OYKVgghhGiN6k3GWuur61mvgX95LCIhhBDiFCM9cAkhhBBeJslYCCGE8DJJxkIIIYSXSTIWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCy1p0oAghhBCe48jPp2TjRko3bqRkw0achQUEnXc+IeMuxtKunbfDE40gyVgIUclptaLMZpRS3g4FcMVjPXAA6779WPfvo3zvPqz79mHLzABAKQMYDGBQKJR73gDqqHVKgTKA0YjBYkH5+LgfFgwWH1TlMgsGHx+UxbXe4ONebjKhnU5wakAfmdcatLOG5655ZbFgTojHkpiIOSERY2BAk86HLS2NkvUbKNm4gdL1Gyjfvdu1wmzGr1s3jGHhZL35JllvvIHfmWcScsk4gsaMwRQW1sS/hIvWmrJt2yj67TeKlv6GPTOTgIEDCBg2jIChQzG1aeOR43ibs6yMvC++pGzLFuKee7ZFjqlcvVm2PBm1STSV7fBhHPn5+Hbu7O1QTnraZiP9iSfI+/wLUArl54fB3x9Dtakfyt8fg59/9XXu5caAAAxHPwIDXVN/f5Sh9lYxR14e5Ucl3PL9+7AlJYPTWbmdOS4OS8eOmGNjAEW1xOh0up+757VG66PW2exoqxVtteK0WtHl5ejycpzWcnS5+7l7fXMwhodjTkzAkpDomrqTtKVtIqaoKJTReORv4nRi3buXkg0bKNmwkZIN67GnusbgMQQE4NenD/79+uLXrx9+PXti8PMDwJaeTsG335L/9UJXsjabCRwxgpBx4wg8axQGi6VRMTuLiylevbpaAkYpfHv2xBwbS8maNTjy8gDw6dqVwL8NI2DYMPz69sXg4+OR89ZSnMXF5M6ZS/bs2TiysvDr25e278zEENC0H1EVlFIbtNb9a1wnyVicLGwpKRSvW0fJunWUrFuP7dAhMBjo8NUCSchN4CgqJuX22ylesYLQ8eMxRbbBWVKKs6QEZ2kpztISdEmJa1lpleUlruUNpfz9MQT4Y/Q/kqi104F1/wEc2dlHtrNYsLRvj6VjR3w6dnRPO2Bp3x6Dv39znIJjaKcTbbNVJmttt7tK2SiUwV0CV8r1w6WyNO4ugVesMxjQpaVYk5OxJSVhTUrClpSMLTkJa1IyttRUcDiOvG+zGXN8PObERJTBQOnmzTjy8wEwRrbBv19//Pv2xb9/P3w6d0aZ6q7Y1FpTvmMH+V8vJP+7b3FkZmEIDiZ4zBhCxl2MX9++tf5Ash46RNHS3yhaupSSdevQNhuGwEAChg0jcNQoAkcMxxQRUXmuyrZtp3jlSopXrqRk0yaw2VC+vvgPGEDAsKEEDhuGpVOnBtW4aKcTe1YW9rQ0bGlp2FJdU3t6GigDQeefR9BZZ3n0s+AoLCT3k0/Ief8DHHl5+A8ZTJubb8Z/wACP1hJJMhYnHa01tqQkV+Jd60rAttRUAAwhIfj3d30xZb35JgEjhpPwyitejrhmzuJist97j5KNm4i+/z58u3TxdkjV2DIySJo+nfKdu4h9/DFCr7yyUa/XWqPLynAWF1d7OKo+Lyo+Zn3FQ6Px6dABS4eOWDp2wKdjR8zx8dVKiK2VttuxpadjO3TIlZwrknRSEs7yMvzOPBP/vv3w79/PlaCbkBS0w0Hx6t/JX/g1hT8vRpeWYo6PJ3jcxYSMG4clIYGSDRvdpd+lWPfvB8DSoQOBI0cSOGoU/v36oszmeo/lLC6meN06ileuonjlSqz79gFgiopyVWcPG4bP6Z2wHz6MLS0dW1qqK/G6k67t8GGw2art0+DvjykuFmdBIfaMDJSfH0Fnn03w2IsIHDYM1cjSfgV7bi65H31Ezkcf4ywsJHDkSCKm/wP/Pn2Oa3/1kWR8krNnZpL31VeYo6MxJyRgjk/AFNmmzmo/byj9808yX/8/DH5+GEOCMQQFYwwOPjIfEowxKAhDcEjlfMU/Iq011v0H3KVe18N++DDgqtrz798f/wED8B84AJ/TT6987xmvvkr22zPp8PVXJ1TpWDud5H/1NZmvvII9M9NVCiwvJ+quOwm7/voT4m9XvncvSVOnYc/LI+HVVwgcMcLbIYkW4CwupnDxYvK/Xkjx77+D04ny80OXlqLMZvwHDCBw1EgCR470yEVgttRUiletomjlSopXrcbpLu1XMhoxRUdhjo3DHBuLOTYGU2ysaz7OtcwQFIRSCu10UrJ+PQXffU/hjz/iyM/HGBJC0HnnETx2LP79+zXoh5w9K4uc998n99PPcJaUEDR6NBHT/4Ff9+5Nfr91kWR8kku6+Z8ULVlSbZny8XFVaSXEY0lIwJyQeGQ+MRFjUFCLxqidTg6MvwrroUOYoqNw5hfgKChAl5fX+Trl54cxOBhts+HIyQFcVXIBAwa4ku+AAVhOO63WUoE9N5e9544+oUrHJevWcfiZZynbtg3f3r2Ivu8+LO3akfbgQxQtWULA3/5G3DP/xRQZ6b0Y168n6V+3oMxmEt96C78ezfslJE5MtsMZFHz3HbbkJPyHDCFgyNAmX2RWF+1wULZtG7akJEwxMZhjYzFFRtZb5V7jvqxWilatciXmX35Bl5Rgiooi+IILCB47Ft8e3Y/53rAdPkz2e++RN+9ztNVK8AUXEPGPaS32Q16S8Ums8NdfSf7nv4i8/XaCzhuNLTnZ1QaVnOJqh0pxzTsLCqq9zhASgiU+Ht/evYi5//7jrsZpqPzvviP1zruIffYZQi+9tHK5s7wcZ4ErMTsKCtzzhTgK8qvN49SuarmBA7C0b9+oKrkTpXRsTUoi44UXKfzpJ0wxMUTdeQfBF11UWQrWWpM3Zw6Hn30OQ0AAsf99mqBRo1o8zoIffyT1nnsxx8eT+M5MLAkJLR6DEJ7kLCmhaOlS8r/9jqLly8Fmw9yuLSEXXeT6N+jjS/Y775A/fz7a6SRk3Dgipk3Fp0OHFo1TkvFJyllayr6LxmII8KfD/Pl1ttc4CgpciTopGVtyMraUZKwHDlK8ahVRd91JxJQpzRantlrZe9FYDP7+dJj/ZYu393m7dOwoKiL7rbfI+eBDMJmImDqFiMmTK69uPVr5nj2k3HU35Tt2EHbNNUTdczcGX98WiTX7/ffJeO55/Pr0IeGN//PYLS9CnCgc+fkU/vwz+d9+R8maNa7bzZRCmUyEXHE5EVOmYkmI90psdSVjuc/4BJb11tvYUlNp99GH9V44YQwOxtitG77dulVbnnTzP8l6cwbBF4/DHB3VLHHmzpmLLSmJxHdmeuXCG1NYGGHXXUf2zJmU3byrxUrH2uEg74svyXztNRw5OYRceimR/7kdc3R0na/z6dSJ9vPmkvnSy+R88AEl69YS9+KLzXpxl3Y6yXjuOXI++JCg884j7vnnWuwHgBAtyRgSQuiVVxJ65ZXYMjJcbct5eYRedRXmmBhvh1crKRmfoMr37WffJZcQcuEFxD333HHvx3roEPvGXkzQ+ecT/8LzHozQxVFUxN7R5+HTpQttZ8/yWmcR9txc9p5zLgEjR7RI6bh49WoOP/Ms5bt24devH9H33Ydfzx6N3k/R8hWk3n8/zoKCIxd3efgcOsvLSb3nXgoXLSLs79cTfe+9p8TVykKcaOoqGXv/kk5xDK016U8+gcHXl6i7727Svixt2xJ+42QKvvmGkg0bPBThEdnvvYcjN5eoO+/0aq9NprAwwq6/nsIfF1G2a1ezHad8/36Sbv4nhybfiLO4mPhXX6Xdxx8dVyIGCBz+Nzou/JqAoUM5/N9nSJr2D+xZWR6L15GXx6Ebb6Jw0SKi7ruXmAcekEQsREPYrVCa22KHk5LxCajg++9JueNOoh9+iPBrr23y/pwlJey9aCzG0FA6fPG5x76MbRkZ7D1/DEFnjSL+5Zc9ss+maO7Scf4335B6/wMYfHyImP4Pwv/+d4/1MKS1JvfTT8l4/gWPXdxlTU4hado0bElJxD3/HMEXXOCRWAHXF5WtGKwlYC0Gpx0sAeATBJZAMDXvBYMtwl4O5UVQXuCaB1dnHq6+NqtMa1heuQzXuXHYXVOnrQHPHaCdYDCCwVTlUfHcXMMy90MpV6z2MnBYXVO79ajn5a6Ho/zIvHaC2R/MfmDxB3OAez6gyvKKeX/XNiZf9/GO+izYil1TawlYi8BWctR8MThsYPJx7dfkA6aKqe+xy8xV1hlMrlidFefL4ZpWLnMcWVd1mb3MdfzyIigvdM8XuJ5b3csq/tYVzx1W8AuDew947CMlbcYe5MjLI/uDDyjdtJm4Z/6LOTbWs/svKuLwM8/i260bYRMnemSfBn9/ou+5m5T/3EHe5597bL9Z//cG2m4n8vbbPbK/pqooHWfPnEn5P3fjc/rpHtt32a5dpD38CH5n9ibh1Vc93gevUorwa68lYOBAUu68i+TpNxM2/hKipk3EENHW9aXQiJqH0q1bSZo+HV1upe2s9/AfMODYjbSGkhzIPQC5+13TwnT3F2lNX6IlR75onfa6AzBaXEnZJxAsQa5pRaKuuszk40pEDqsrETlsrnmH9chyh/XIcmfFtg7XMYxm19Tkc2S+clox73NkHlzxV34hFx75kq6cur+Ynba632NrYTC7PluOxnYBqlw/Bur7LFR7icGV6I0m148AWynghQKh2f3D0SfQ/ZkMgtDEKp9R9zrf0BYL6dQuGR/6Hb6/2/WLyzfY/QdwT31Dqj23Ww3kLFxO7lc/4ywpdd3nm5BAu48/8ugVqYefeZacDz+k/dw5+PXq5bH9aq05NGky5Tt20PHHH5occ/m+fey7eBxhV19NzEMPNjU41z9oY/29+9SnonQcOGpk9dK61lB0GDJ3QtYuKEh1/8IPqPIIdE/9q8wH4LQZ2D/xGhwFBXRcML/p9wdXJMH8JChIgfyUavPO7CQyVhSRuysAo68Dk4/T9WVpMqNMrkSjzBYw+4DJB2XxBbMvyuzr2sZgoGTdOgyhIbSd8X/4RFiqJ9zcA5B70DW1FlaPzTfU/YUUcOT8VJ4nd4nJUrG8yrkyGN1Jrkoyq1riqLasyrqKL2KD6dhEWrmsYnmVZKsMRyXuKvP2GpbpI91OogxHfgxU/jgIPPaLuOoys++Rv13lVB87PWYdrnirll6NZneJ1lz7c5Qr5soSoLuE57AdWyqs+tDaXYp0P4zuEqXJ4poaLUc993F384nrx4+91P2jq6TKD7CSIyVaW2n1eae9yueiysNc8W/Iv/p8RWm66r8Fh61Kib30SJKu+txeBrYy1w+kihoBZaxeO6AMx9YWVCwzWqr/XQ3eaaqRW5tq4nTCO2dBfjJEd3d/aRS4pmUFrg8BYC83kLMjgNzdATjtiqDEMtp0L8RhM5O0NBzfdpG0feMlDO37NarkUpOynTvZf/kVhF55JbGPP+aBN3n0/nex//LLCb1qPLGPPtqkfSXdcgslq3/ntJ9/whQeXvuGTieUZLsSTUFqlelR8/ZSCIyB0LY1PNpBSMKRL8S6OB1kPPsE2R99TsfHx+NjyYKsnZC5C8qr9PyjDK5qrHpoDWlrQsk/4Efb860EtPN1f3lWJAZT9Xmjxf28SvIwmFxfnAUprs9bxfutymiB4HjX+3RPiw5ayV+xA11egraWgq0MbXV/KdnK0VpXzwNaoQ0mUBZMQWZiBpdhdqRSreRh8nWdz7D2NTzaub5IW0rFF7HR3OR/O/VyOtxJ2V0de4KMSiVOLZKMa7J1AXw+CS6dAWdec8xqe+Zhct59h5x5X6LLygke0Z82V56FT3SAK2nnHqRg0Y+kfF9IQEw5iRf6oDqfA51GQ8dR4BfaqHC008nBa6/DeuAAp/3wPcbQxr2+odKfeprcTz+lw5df4HvGGcduUF7kKkEVpFVpz7JV+xVesuMgB5/8jMjLBtNmbJ/qv9Jtxa7XViTbwrRjq78MJgiKheA49yPe9Wu1IBnyDrke+cnHVn/VlKx9giB7rzvh7oTsPdiLy9n7TTSBceXEjzZDZBdo07n6NCjW9b4q27fcVbMV7V7WIrAWk7d4HWmzf6HNBd2JHN3Btd5pq1LyslWpXrVVWXdU1asyHHmvIQnVki4hCeDf5kgJpSG0dl1cUpDqOsdHT63FrvNTNdmGtoPA6MYdRwjhMZKMj+aww5uDXEnh5lXVqizs2dlkvzeL3M8+Q5eVEXzhhbS5eTo+nTrVuKvcD98h/b8vE9I7gtjeh1DWfFf1SeJA6HSu6xHTq94vwLwv55P24IPEPv00oVdcXn2l1q4vdqOlyb/oHXl57B0zBktCNO0emoiqqL7M2e+aFmfW+Xqt4eAvEViLTHQam4HBdNTnx+TrTrTx1ZNt5XwcBETWX03kdLiSSkVyzjsEeQdd1at5h1yJvjJZK1d7T5surkQb2YWMb7aQPec7Oi78+rjbjst27uLAVVfh17cPbd99V65CFkI0iSTjo234AL65DSZ+Cl0vAlyDMWS/N4vcOXNcfZaOvYg206fj07FjvbvLmjGDzNf+R/ikG4i+ZhTs/hn2LIa0za4NAqKg0zmuxJw4yNUeUpbnKtmU5uLITGfvfe9jifSn3U3dUeUV69zTsjxX4lHGKm1cR7dzHj0NcG0H7vbB/ZBzAHL3k7vdSfq6UOIG5xLSvsyVLMM7uB5h7mlwgqtd6airOQuXryX5nkeJeeAuwsZfcezVni1V/eewu5J1eYErZkv14dRqbTtuIGdxMfvHX4WjsICOCxa0mkHThRDeI1dTV2UrhaXPQsIA6HIhtowMct57j9w5c9E2GyEXX0zE9H80qs/SiOnTsWdlk/P+B5giI4m46WE452EoyoA9v7gS864f4Y/Panx9xroQHMX+xJydi0oucl056xfqqr70C3NdVGPxd8VepQq18oKZgtQqVa3u9VXbCY0Wd1VlB2g/jNBR7cnLXUDG7jACX/sWY0gdbb5VaLudjLf+haVDB0KvuQGOo3N3jzGaXKXhWlS9srrNzTc3qnSstSbt8cexHjhA21mzJBELIZpdq0jGWmucxSU4C10DDzgLC3AUFuIsLKzyvMg13b8JZ1I5jhAzzs8vwJaaWtlxeJvp/ziuIcOUUkQ/+ACO3BwyXngRY1g4oZdfBoFRcObVrofTAambIO0P15XavqHgF0bp/sPkzb2L8L9fj+/993vqhBy54tFpd7cTHqliVUDM00M5MGEi2e/OJurOOxu027z587Hu20f86/87rlFWWlr4pBvI/egjsmbMaFTpOP/LLylY+A1tbr2FgMGDmjFCIYRwaRXV1Llz5pL+2GN1bqN8fTEGBWKwZmAMDMDQsT/GoCBM0dGEXT0RS9u2TY5DW60kTb+Z4jVrSHj9dYLOPqvu7R0ODoy/CntmJh1/+B5jYGCTY2iM1PsfIP/bb13tqvXUBDhLSth7/hjX7VyffuLV3rYaI+PlV8h+550Gtx2X7dzJgasm4N+vL4nvvCPtxEIIj2n1bcZlO3dRvHIlhqBAjEHBGIODMAQFuQeyD8YYGOgaQvDXp2DZCzDtN4g70yPHPpqjqJhDkyZRvns3bWfPwr9v31q3zfn0Uw4/8STxL79E8IUXNks8dbFnZbF3zAX49e1D4ttv15lgs956i8xXX6PdJx/j369fC0bZNI1pO3YUFXPgyitxFBdJO7EQwuNafd/Uvl06E3HjZMLGjyd4zPkEDB2KX8+eWNq3xxQe7krERRmw+k3oflmzJWIAY2AAiTPfxhwbS9L0m2vtJ9melUXmK68SMHQIQZ7sprARTG3a0OZf/6J42XKKliytdTt7bi7Z77xL4Nlnn1SJGI6M6FTww4+U795d63Zaa9IfewzroUPEv/iSJGIhRItqFcm4QZa96OrF5ayHmv1QpvBwEt99F4OvL0lTpmJLSTlmm4wXXsBZVkb0Qw97tco3/LprsZx2GoefeQZneXmN22TNmIGztJSoO/7TwtF5RvjkSRj8/MiaMaPWbfI+/5yCb78l8tZbCBg0sAWjE0KIUyUZ5x6A9bOgz3XQpub7hT3NkhBP4jvv4Cwr49CUqdhzj4z+Ubx2LflfLyTiphvx6djwq7abgzKbiXnwAWxJSeTMnn3MemtSErmfzSH0istrvdf6RFetdLxnzzHry3bs4PBTTxMwdCgR06Z5IUIhxKnu1EjGS591XU088t4WPaxvl84kzngTW2oqSdP+gbO4GG2zcfjJJzHHxdHmH/9o0XhqEzB0KEGjR5P19kxsaWnV1mW++hrKaKTNLbd6KTrPqCwdv1m9dOwoKibl9v9gDAkh7oXn5YItIYRXtP5kfHgb/DEHBk6DkPgWP7x/v37Ev/IyZdu2kXzbv8l+bxblu/cQ/dBDGPz8Wjye2kTdey84nWS88ELlstK/tlLw3XeE33AD5ugoL0bXdEdKxz9Ulo611qQ/+ijWQ4eIe+lFTBERXo5SCHGqav3J+NenXP0X/8177Z1BZ59N7BOPU7xyJZmvvkrgWWfVe9tTS7MkxBMxdSoF3/9A8Zq1aK3JeOlFjKGhREy5ydvhecTRpeO8eZ9T8N13RN52GwEDpZ1YCOE9rTsZJ62Fnd/BsNvAv2G9TDWX0CuuIOqeezDFxhL94ANejaU2EVNuwhwXx+Gnn6Z42TJKVv9Om5unYwwK8nZoHlG1dJz/7XccfvppAoYNI2LaVG+HJoQ4xbWK+4xrpDW8P9Y1du1tm4700+xlWusTusOMgp9+IuW2f6P8/TGFhdHxh+8xWCzeDstjKu47dpaUYIqKosNXC+oeAlIIITyk1d9nXKO9v8DBFTDi7hMmEQMndCIGCBo9moChQ9AlJUTefnurSsTgKh2HT7oBjEbiX35JErEQ4oTQOkvGTifMHOka7eiWDa7Rh0SD2Q5nULRkCaFXjUe1wrFvtdY4srIwRUZ6OxQhxCnk1Bu1adsCSP8TLpspifg4mKOjCJs4wdthNBullCRiIcQJpfUVexw2+PVpiOoGPa/0djRCCCFEvVpfyXjTx5CzF66eU23YQCGEEOJE1bpKxrZS+O05SBwEncd4OxohhBCiQVpXyXjtTChMgyvegxP8qmUhhBCiQoNKxkqpMUqpnUqpPUqp+2pY31YptUQptUkp9adSquUH5y3Ng+UvQ6fR0H5Yix9eCCGEOF71JmOllBF4A7gA6AZcrZTqdtRmDwHztNZ9gInAm54OtF6rXnfdynTOIy1+aCGEEKIpGlIyHgjs0Vrv01pbgTnAJUdto4Fg93wIkOq5EBug8DD8/ib0uAJie7XooYUQQoimakgyjgeSqjxPdi+r6jHgOqVUMvA9UON4e0qpaUqp9Uqp9ZmZmccRbi22fQ0OK5z1oOf2KYQQQrQQT11NfTXwvtY6AbgQ+Egpdcy+tdYztdb9tdb9Iz3Z6cKgafCvtRBxmuf2KYQQQrSQhiTjFCCxyvME97KqbgLmAWitVwO+QBtPBNhgkoiFEEKcpBqSjNcBpyulOiilLLgu0Fp41DaHgHMAlFJn4ErGHqyHFkIIIVqvepOx1toO3AIsArbjump6q1LqCaXUOPdmdwJTlVJ/AJ8Bk7S3RqAQQgghTjIN6vRDa/09rguzqi57pMr8NkBu7hVCCCGOQ+vqDlMIIYQ4CUkyFkIIIbxMkrEQQgjhZZKMhRBCCC+TZCyEEEJ4mSRjIYQQwsskGQshhBBeJslYCCGE8DJJxkIIIYSXSTIWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF4myVgIIYTwMknGQgghhJdJMhZCCCG8TJKxEEII4WWSjIUQQggvk2QshBBCeJkkYyGEEMLLJBkLIYQQXibJWAghhPAyScZCCCGEl0kyFkIIIbxMkrEQQgjhZZKMhRBCCC+TZCyEEEJ4mSRjIYQQwsskGQshhBBeJslYCCGE8DJJxkIIIYSXSTIWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF4myVgIIYTwMknGQgghhJdJMhZCCCG8TJKxEEII4WUmbwcghBCiaWw2G8nJyZSVlXk7FAH4+vqSkJCA2Wxu8GskGQshxEkuOTmZoKAg2rdvj1LK2+Gc0rTWZGdnk5ycTIcOHRr8ugZVUyulxiildiql9iil7qtlm6uUUtuUUluVUp82OAIhhBBNUlZWRkREhCTiE4BSioiIiEbXUtRbMlZKGYE3gNFAMrBOKbVQa72tyjanA/cDw7TWuUqpqEZFIYQQokkkEZ84judv0ZCS8UBgj9Z6n9baCswBLjlqm6nAG1rrXACtdUajIxFCCCFOUQ1JxvFAUpXnye5lVXUGOiulViqlfldKjfFUgEIIIU58gYGB3g7hpOapC7hMwOnAKCABWKaU6qm1zqu6kVJqGjANoG3bth46tBBCCHFya0jJOAVIrPI8wb2sqmRgodbaprXeD+zClZyr0VrP1Fr311r3j4yMPN6YhRBCnKC01tx999306NGDnj17MnfuXADS0tIYMWIEZ555Jj169GD58uU4HA4mTZpUue0rr7zi5ei9pyEl43XA6UqpDriS8ETgmqO2+Qq4GpitlGqDq9p6nwfjFEII0QCPf7OVbakFHt1nt7hgHr24e4O2nT9/Pps3b+aPP/4gKyuLAQMGMGLECD799FPOP/98HnzwQRwOByUlJWzevJmUlBT++usvAPLy8jwa98mk3pKx1toO3AIsArYD87TWW5VSTyilxrk3WwRkK6W2AUuAu7XW2c0VtBBCiBPTihUruPrqqzEajURHRzNy5EjWrVvHgAEDmD17No899hhbtmwhKCiIjh07sm/fPm699VZ+/PFHgoODvR2+1zSozVhr/T3w/VHLHqkyr4E73A8hhBBe0tASbEsbMWIEy5Yt47vvvmPSpEnccccd/P3vf+ePP/5g0aJFvPXWW8ybN49Zs2Z5O1SvkL6phRBCeMzw4cOZO3cuDoeDzMxMli1bxsCBAzl48CDR0dFMnTqVKVOmsHHjRrKysnA6nVxxxRU89dRTbNy40dvhe410hymEEMJjLrvsMlavXk3v3r1RSvH8888TExPDBx98wAsvvIDZbCYwMJAPP/yQlJQUJk+ejNPpBOCZZ57xcvTeo1w1zC2vf//+ev369V45thBCtCbbt2/njDPO8HYYooqa/iZKqQ1a6/41bS/V1EIIIYSXSTIWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF4myVgIIcRJw263ezuEZiHJWAghhEdceuml9OvXj+7duzNz5kwAfvzxR/r27Uvv3r0555xzACgqKmLy5Mn07NmTXr168eWXXwIQGBhYua8vvviCSZMmATBp0iSmT5/OoEGDuOeee1i7di1DhgyhT58+DB06lJ07dwLgcDi466676NGjB7169eL111/n119/5dJLL63c788//8xll13WAmejcaQ7TCGEaE1+uA/St3h2nzE94YJn691s1qxZhIeHU1payoABA7jkkkuYOnUqy5Yto0OHDuTk5ADw5JNPEhISwpYtrjhzc3Pr3XdycjKrVq3CaDRSUFDA8uXLMZlMLF68mAceeIAvv/ySmTNncuDAATZv3ozJZCInJ4ewsDD++c9/kpmZSWRkJLNnz+bGG29s2vloBpKMhRBCeMT//vc/FixYAEBSUhIzZ85kxIgRdOjQAYDw8HAAFi9ezJw5cypfFxYWVu++x48fj9FoBCA/P58bbriB3bt3o5TCZrNV7nf69OmYTKZqx7v++uv5+OOPmTx5MqtXr+bDDz/00Dv2HEnGQgjRmjSgBNscli5dyuLFi1m9ejX+/v6MGjWKM888kx07djR4H0qpyvmysrJq6wICAirnH374Yc466ywWLFjAgQMHGDVqVJ37nTx5MhdffDG+vr6MHz++MlmfSKTNWAghRJPl5+cTFhaGv78/O3bs4Pfff6esrIxly5axf/9+gMpq6tGjR/PGG29Uvraimjo6Oprt27fjdDorS9i1HSs+Ph6A999/v3L56NGjefvttysv8qo4XlxcHHFxcTz11FNMnjzZc2/agyQZCyGEaLIxY8Zgt9s544wzuO+++xg8eDCRkZHMnDmTyy+/nN69ezNhwgQAHnroIXJzc+nRowe9e/dmyZIlADz77LOMHTuWoUOHEhsbW+ux7rnnHu6//3769OlT7erqKVOm0LZtW3r16kXv3r359NNPK9dde+21JCYmnrCjW8kQikIIcZKTIRTrd8stt9CnTx9uuummFjleY4dQPPEqzoUQQggP6tevHwEBAbz00kveDqVWkoyFEEK0ahs2bPB2CPWSNmMhhBDCyyQZCyGEEF4myVgIIYTwMknGQgghhJdJMhZCCCG8TJKxEEKIFld1hKajHThwgB49erRgNN4nyVgIIYTwMrnPWAghWpHn1j7HjpyGD87QEF3Du3LvwHvr3Oa+++4jMTGRf/3rXwA89thjmEwmlixZQm5uLjabjaeeeopLLrmkUccuKyvj5ptvZv369ZhMJl5++WXOOusstm7dyuTJk7FarTidTr788kvi4uK46qqrSE5OxuFw8PDDD1d2wXmik2QshBCiySZMmMDtt99emYznzZvHokWLuO222wgODiYrK4vBgwczbty4aqMz1eeNN95AKcWWLVvYsWMH5513Hrt27eKtt97i3//+N9deey1WqxWHw8H3339PXFwc3333HeAaUOJkIclYCCFakfpKsM2lT58+ZGRkkJqaSmZmJmFhYcTExPCf//yHZcuWYTAYSElJ4fDhw8TExDR4vytWrODWW28FoGvXrrRr145du3YxZMgQnn76aZKTk7n88ss5/fTT6dmzJ3feeSf33nsvY8eOZfjw4c31dj1O2oyFEEJ4xPjx4/niiy+YO3cuEyZM4JNPPiEzM5MNGzawefNmoqOjjxmn+Hhdc801LFy4ED8/Py688EJ+/fVXOnfuzMaNG+nZsycPPfQQTzzxhEeO1RKkZCyEEMIjJkyYwNSpU8nKyuK3335j3rx5REVFYTabWbJkCQcPHmz0PocPH84nn3zC2Wefza5duzh06BBdunRh3759dOzYkdtuu41Dhw7x559/0rVrV8LDw7nuuusIDQ3l3XffbYZ32TwkGQshhPCI7t27U1hYSHx8PLGxsVx77bVcfPHF9OzZk/79+9O1a9dG7/Of//wnN998Mz179sRkMvH+++/j4+PDvHnz+OijjzCbzcTExPDAAw+wbt067r77bgwGA2azmRkzZjTDu2weMp6xEEKc5GQ84xNPY8czljZjIYQQwsukmloIIYRXbNmyheuvv77aMh8fH9asWeOliLxHkrEQQgiv6NmzJ5s3b/Z2GCcEqaYWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBAtrq7xjE9FkoyFEEKcsux2u7dDAOTWJiGEaFXS//tfyrd7djxjnzO6EvPAA3Vu48nxjIuKirjkkktqfN2HH37Iiy++iFKKXr168dFHH3H48GGmT5/Ovn37AJgxYwZxcXGMHTuWv/76C4AXX3yRoqIiHnvsMUaNGsWZZ57JihUruPrqq+ncuTNPPfUUVquViIgIPvnkE6KjoykqKuLWW29l/fr1KKV49NFHyc/P588//+TVV18F4J133mHbtm288sorx3t6AUnGQgghPMCT4xn7+vqyYMGCY163bds2nnrqKVatWkWbNm3IyckB4LbbbmPkyJEsWLAAh8NBUVERubm5dR7DarVS0SVzbm4uv//+O0op3n33XZ5//nleeuklnnzySUJCQtiyZUvldmazmaeffpoXXngBs9nM7Nmzefvtt5t6+hqWjJVSY4DXACPwrtb62Vq2uwL4AhigtZaOp4UQooXVV4JtLp4cz1hrzQMPPHDM63799VfGjx9PmzZtAAgPDwfg119/5cMPPwTAaDQSEhJSbzKeMGFC5XxycjITJkwgLS0Nq9VKhw4dAFi8eDFz5syp3C4sLAyAs88+m2+//ZYzzjgDm81Gz549G3m2jlVvMlZKGYE3gNFAMrBOKbVQa73tqO2CgH8DLd6Pmdaa7WmFdIsLbulDCyGEcKsYzzg9Pf2Y8YzNZjPt27dv0HjGx/u6qkwmE06ns/L50a8PCAionL/11lu54447GDduHEuXLuWxxx6rc99Tpkzhv//9L127dmXy5MmNiqs2DbmAayCwR2u9T2ttBeYANVX6Pwk8B3hm5OhGmL3yABf+bzlJOSUtfWghhBBuEyZMYM6cOXzxxReMHz+e/Pz84xrPuLbXnX322Xz++edkZ2cDVFZTn3POOZXDJTocDvLz84mOjiYjI4Ps7GzKy8v59ttv6zxefHw8AB988EHl8tGjR/PGG29UPq8obQ8aNIikpCQ+/fRTrr766oaenjo1JBnHA0lVnie7l1VSSvUFErXW33kkqkYa3S0agIV/pHrj8EIIIah5POP169fTs2dPPvzwwwaPZ1zb67p3786DDz7IyJEj6d27N3fccQcAr732GkuWLKFnz57069ePbdu2YTabeeSRRxg4cCCjR4+u89iPPfYY48ePp1+/fpVV4AAPPfQQubm59OjRg969e7NkyZLKdVdddRXDhg2rrLpuqnrHM1ZKXQmM0VpPcT+/Hhiktb7F/dwA/ApM0lofUEotBe6qqc1YKTUNmAbQtm3bfg39ldQQV721muzichbfMbLeiwOEEKI1kfGMW97YsWP5z3/+wznnnFPj+uYYzzgFSKzyPMG9rEIQ0ANYqpQ6AAwGFiqljjmg1nqm1rq/1rp/ZGRkAw7dcJf1jWdvZjFbUws8ul8hhBCiQl5eHp07d8bPz6/WRHw8GnI19TrgdKVUB1xJeCJwTcVKrXU+UFmur6tk3Jwu7BHLo19vZcGmFHrEh7TkoYUQQhyHk3E849DQUHbt2uXx/dabjLXWdqXULcAiXLc2zdJab1VKPQGs11ov9HhUxyHE38zZXaNY+Ecq91/QFZNROhc7XqtSVvHapteYff5s/M3+3g5HCNEAWuuTromutY5nXF/zb00alLG01t9rrTtrrU/TWj/tXvZITYlYaz3KW/cYX9onnszCclbtzfbG4VuNL3Z/wbbsbaxLX+ftUIQQDeDr60t2dvZxJQHhWVprsrOz8fX1bdTrWlUPXGd1jSTY18SCTSmM6OzZNulThdVhZWXKSgBWpq5kZOJIL0ckhKhPQkICycnJZGZmejsUgevHUUJCQqNe06qSsY/JyEW94vhqUwpPXWonwKdVvb0WsS59HSX2EoItwaxKXeXtcIQQDWA2myt7jRInp1bXsHpZn3hKbQ5+3nbY26GclJYkLcHP5MdNPW/iYMFBkguTvR2SEEK0eq0uGfdvF0Z8qB8LNqXUv3EL01qTUZLh7TBqpbVmadJShsQO4azEswCkdCyEEC2g1SVjg0FxaZ84lu/OJLOw3NvhVPPpjk85/8vzSS9O93YoNdqRs4PDJYcZlTiK9sHtiQuIq2w/FkII0XxaXTIGuPTMeJwavjmBusd0aiefbP8Eu9POsuRl3g6nRkuTl6JQDE8YjlKKofFDWZO+BpvT5u3QhBCiVWuVyfj06CB6xAfz1eYTp6p6VeoqkgqTMCojvyX/5u1warQ0aSm9InvRxs/Vh8uwuGEU24r5I+MP7wYmhBCtXKtMxuAqHf+ZnM+ejKJatymxlbA5Y3OLxDNnxxwifCO44vQrWJu2ljJ7iw9uVaf04nS2ZW9jVOKoymWDYgdhVEZpNxZCiGbWapPxuN5xGBR8XUvpOLcslxsX3cj1P1zP+vTm7aMkpSiFZcnLuKLzFZzT9hzKHGWsTV/brMdsrIqq84oLtwCCLEH0iuzFylRpNxZCiObUapNxVLAvwzq1YcGmlGN6pUkvTueGH29gT94egixBvPfXe80ay7yd81BKMb7zePrH9MfP5HfCtRsvSVpCYlAiHUM6Vls+NG4o27O3k1OW46XIhBCi9Wu1yRhc9xwn55ay4WBu5bKDBQf5+w9/J6MkgxnnzmBy98msSFnBjpwdzRJDuaOc+bvnc1biWcQExGAxWhgcO5hlyctOmK7rSmwlrElbw6jEUcf0bTssbhgazerU1V6KTgghWr9WnYzP7x6Dn9lYec/xjpwd/P2Hv1NmL2PW+bMYEDOACV0nEGAOYNaWWc0Sw08HfiKvPI+JXSdWLhuZMJK04jT25O1plmM21qrUVdictmpV1BW6RXQjxCdE2o2FEKIZtepkHOBj4rzu0Xz7ZxprUtdz4483YjFaeP+C9+kW0Q2AYEswV3W5ikUHF3Go4JDHY5izYw7tg9szKGZQ5bLhCcMBTpiq6iVJSwi2BHNm1JnHrDMajAyJHcKq1FUnTEleCCFam1adjME1klOR4S9u/mU6EX4RfDjmw2PaRa8/43qMysj7W9/36LG3Zm/lz6w/mdh1YrXq3yj/KM4IP+OESMYOp4NlycsYnjAcs8Fc4zbD4oeRVZrFrlzPj+HpDVprSmwl3g5DCCEqtfpkXGJaj3/iB1icMbw/5n1iA2OP2SbSP5JLOl3CV3u+IrPEc6OezN0xFz+TH+NOG3fMuuEJw9mcuZn88nyPHe94/JH5B3nledVuaTra0LihAK3mqurPd33O2Z+ffcL2hCaEOPW06mQ8b+c87l9xH23MXcjZeyMmgmvddnL3yTi0g4+2f+SRY+eX5/P9/u+5qONFBFmCjlk/MmEkTu30eneTS5OXYjKYGBY3rNZtovyjOD3sdFaltI5244V7F1JsK2b2X7O9HYoQQgCtOBm/u+Vdnvz9SYYnDOf5Ya9jtfrw419ptW7fNrgt57U7j3k751FgLWjy8b/a8xXljnImdplY4/oebXoQ7hvOshTvVlUvTVpK/+j+Nf5gqGpY3DA2Zmw86at3Dxcf5o/MPwgyB/HFri88WhMihBDHq9UlY601L294mdc2vsaFHS7k1bNepX+7KDq2Cah3JKebet5Esa2YuTvmNikGp3Yyd+dc+kT1oUt4lxq3MSgDf4v/GytSVuBwOpp0vON1IP8A+/P311lFXWFo3FBsThvrDzdvBynNbfGhxQA8O+JZHNrBrL+a5yp6IYRojFaVjB1OB4+vfpzZf81mQpcJPDP8GcwGM0opLu0Tz+/7ckjJK6319V3DuzIsfhgfb/+4Sd1VVvRDXVupuMLwhOHkl+fzZ9afx32spqjoI7shybhvdF98jb5er1ZvqsUHF9MptBMjEkZwUceL+HzX52SVZnk7LCHEKa7VJGOrw8o9y+7hy91fMq3XNB4c9CAGdeTtXXpmPAALN9c9ktNNPW4ipyyHr/Z8ddyxzN0xl3DfcEa3G13ndkPjhmJURq9dVb0kaQmdwzoTHxhf77Y+Rh/6x/Q/qe83zirNYmPGRs5tdy4A03pNw+a08cHWD7wcmRDiVNcqknGJrYRbf72Vnw7+xF397+LWPrce05NU2wh/+rULY8Gm5Drvl+0f3Z/ekb15f+v72J32RseSUpTCb8m/ccXpV2A21nyrUIVgSzB9o/t6JRnnleWxKWNTg0rFFYbFDeNAwQFSik6c0bAa49dDv+LUzsofSe2C23FhhwuZu3OudPcphPCqVpGMv9z9Jb+n/c4TQ5/ghu431LrdpX3i2XW4iG1ptV+gpZTiph43kVKUwo8Hfmx0LBX9UF/V5aoGbT8ifgS7cne1+G02y1OW49TOGnvdqs3QePctTidpVfXig4tpF9yO00NPr1w2tddUyuxlUjoWQnhVq0jG155xLR9d8BGXnX5ZnduN7RmL2aj4qp4LuUYmjqRTaCfe2/IeTu1scBzljnIW7F5Q2Q91Q4xIGAG0fG9cS5KWEOkXWdkTWUN0CO5AbEDsSVlVnVeWx9r0tZzb9txqtSYdQzoypv0YPtvxGblluXXsQQghmk+rSMYGZaBXZK96twsLsDCqSxRfb07F4ay9qtqgDNzY40b25O1hefLyBsfx04GfyC3PZUKXCQ1+TYeQDsQHxrdoMrY6rKxMWcnIxJHV2tXro5RiaNxQ1qStwea0NWOEnrckaQkO7WB0+2Pb8af1mkaZvYyPtnnmHnMhhGisVpGMG+OyPvFkFJazem92nduN6TCGuIC4Rg2vWNEP9eDYwQ1+jVKKkQkjWZO2pklXcDfG+vT1lNhLGlVFXWFY/DCKbEVsydzSDJE1n8WHFhMfGE+38GNrAjqFdWJ0u9F8uuNTr/eIJoQ4NZ1yyfjsrlEE+ZjqvefYbDBzQ/cb2JSxiQ2HN9S739r6oa7K7nCSll96zAVkIxJGUOYoY136uoa/kSZYkrQEP5MfA2MGNvq1g2IHYVTGk6przEJrIatSV3FO23Nq/dtM6zWNYlsxH2//uIWjE0KIUzAZ+5qNXNgzlh//SqPUWndnG5edfhnhvuG8t6X+0nFFP9QXn3bxMeuSckp46aed/O25JQx55lfOffk3XvppJ9tSC9Ba0z+mP34mv8r7fpuT1pqlyUsZEjsEX5Nvo18fbAmmZ5ueJ1XXmL8l/4bdaa/zVrMu4V04p+05fLLtE4/0wCaEEI1xyiVjcF1VXWx18PP2w3Vu52fy49ozrmV5ynJ25uysdbuq/VAHW1z9X5fZHCz8I5Xr3l3D8OeX8MaSPXSLC+a+C7oSHezLG0v2cOH/lnPWi0t59ef9dAvtx/Lk5c0+TOHO3J2kF6c36pamow2NH8rW7K0nzQVPiw8uJsovqt7rCqb3nk6hrZBPt3/aQpEJIYTLKZmMB3UIJzbEt96rqgEmdJmAv8m/zrbjqv1Q70wv5PFvtjL4mV+47bNNHMgu5s7RnVl539nMmjSA6SNP49Opg1n34Lk8c3lPEsP9mblsHyu3RJFanMp93/zM5qS8ZkvKS5KWoFCVYyofj2Fxw9Bofk/73YORNY8SWwkrUlZwbrtz671YrWt4V0YljuKjbR9RZC1qoQiFEAJM3g7AGwwGxSVnxvPO8n1kF5UTEehT67YhPiFM6DKBD7Z9wK19biUxKLHaeqd28tmOuST4dePez7LYnLQHi9HAed2jmTigLUNPi8BgOLadMiLQh6sHtuXqgW3JLbby5R+xvLZ7Pl/vWszcVTbiQ/0Y0yOGC3vG0CcxrMZ9OJyavBIrOcVWsout5LqnOVUedqeTPolhDOoYTrfYYJYmLaVXZC/a+LU57vPXPaI7IT4hrEhZwQUdLjju/bSE5SnLKXeUV/a6VZ/pvaYz8buJfLbjM6b2mtrM0QkhhMspmYzBdVX1W7/t5ds/07hhaPs6t72u23V8vP1j3v/rfR4e8jDganvdlJTH/636jpTyJEpTJtLB187DY7txWZ94wgMsDY4lLMDClKF9WJTdFZ826VwW05sf/krjo9UHeW/FfqKDfRh6WhtKrPZqiTav1EZtBeggXxPhARacWvP9FleHIoH+Rah22xgcej0bD+XSMz4Es7HxlSNGg5HBsYNZnboarXWtF0WdCH4++DPhvuH0jerboO27t+nO8PjhfLDtA6454xoCzAHNHKEQQpzCybhLTBBnxAYzb30SnaICKbM5KLM5XVP7kflym4Myu5N40wi+2LWApP1/Q9uDOJBVzO6MIgLafoNvQDDvXj2Vge0jm5SYhscPZ9Zfs3jj3CCu6JdAYZmNX7Zn8P2WNFbtzSLEz0x4gIUuMUGEB1gID/Ah3N9MeKAPEQEWwvwtRAS6phbTkSR7uKCMNftzmLtjLn+WwS8b2vDz6lX4W4z0axfG4I4RDOoQTq+E0Gqvq8uwuGEsOrCIXbm7ah2ZytvK7GUsS17G2I5jMRqMDX7d9N7Tufb7a5m7cy439rixGSMUQgiXUzYZA1zRN56nvtvOte+uqXM7X7MBH7+BOON+ZVPeQiKslxEZ5MPlA/2ZsXc7N/ScwqAOUU2OZ2TiSN7Z8g6rUldxQYcLCPI1c2mfeC7tU/9ADnWJDvZlXO84fsjcRWJBIu/fdQ3rDuSyZn82a/bl8MKinZXvs2/bMAZ1iKBXYgh+ZiMmg8JoUJiNBowGVfm8vf+ZAPy0bxltLO0xGQ2YDAofkwHTcZS2m8Oq1FWU2ksbXEVdoVdkL4bFDeODrR8wsctE/M3+zRShEEK4nNLJ+O9D2tMtNhiDQeFrNuJrNuBrMuJrNuJnNuJjNuBjMlSWdu/6bSMrU1by+ZVPEGQJ4tUNrzaqH+r69IjoQZhPGMuSl3m8LbbEVsLatLVM6DqBqGBfLuoVy0W9YgHIKbaydn9OZXJ+9ZddtVZ/V+XfIZo3137PS1/EVS4zGxW9E0Jdpe2O4fRrF4a/xXMfM601+7KK2XAglw0Hc8kqKmfcmXGM6RGDj6l66ffngz8TbAlmQMyARh9neu/pXP/D9Xy+6/M6+zsXQghPOKWTscVkYGinhl/IdFOPm1h0YBFzd87l+m7XM3/3fEYljGpwP9T1MRqM/C3+byxPWY7D6WhU1Wp9Vqeuxuq01tjrVniAhTE9YhjTw/U+8kts7M4oxOpw4nBq7E6N3aFxOJ2V83an5sfUoazP/YY7xp6GAR/sDifZxVbW7M9hxm97+b8lezAZFL0SQhjUMYLBHSPo3y6MAJ+Gf+zKbA62pOSz/kAuGw7msOFgLrklrq44Q/zMBFiM/LIjg4gAC1cNSOSagW1JDPfH5rDxW9JvnNPuHMyGukfPqsmZUWcyOHYws/+azVVdrsLP5NfofQghREOd0sm4sc6IOINhccP4aNtHhPqEkluey8SuEz16jBEJI/hm3zdsydrCmVFnemy/S5KWEGwJbtA+Q/zN9G8fXu92cTFjWLN4AV3aZ1QOeFGhqNzO+gM5rNmfw5p92byzbB8zlu7FaFD0jA9hUMdwBneIoH/7MIJ8jyTLzMJyNhx0Jd71B3P5KyUfm8NVTO/YJoBzz4imX7sw+rcPo2ObQACW78ni498P8vZve3nrt72M6hxJ366HKbQV1jumdF2m957OpB8n8eWuL7mu23Wk5ZdyKLuENkE+RAf7EtiIHxVCCFEX+TZppJt63sSNi27k2bXPNrof6oYYGj8UozKyLHmZx5Kxw+lgWfIyhicMP65SYm36RvfFx+jDqtRVxyTjQB8To7pEMaqLqy29xGpnw8Fcft/nqgqftWI/b/+2D4OCHvEhtIsI4M/kPA5mlwCuWote8SHc+LcO9G8XTt+2obXegjaycyQjO0eSmlfKnLWH+GxdEqsK5mMJ9mXzrkjOCCknMqj229dq4nBq/J2nk+jXk1fWvc2bCyNJzas+vnWAxUh0sC9Rwa7kHB3sS1TQkfnoYB+ignzxs3iuhkMI0TpJMm6k/tH96RXZiz8z/2RClwkev60n2BJMn6g+LEtexm19b/PIPv/M+pPc8twm9bpVE1+TL/2j+zdofGN/i4nhp0cy/PRIAEqtDjYeymXNvmx+35fDhgM59EwI4dpBbenXLpwe8cHHtAHXJy7UjzvO68L0s9pz1txHsFh78/JP+3j9l/2M6RHLdYPaMrBDeI1/sxKrnc1Jeaw/kMv6g7lsOphLYbkdo/9g/NttITF+M1P+dhWnRQWSW2zlcEEZGYXlrmlBOZuT8kjPL6PcfuyQm8G+JmJCfEkI8ycxzM81DXdPw/wJ8a/5B9KMP2bw/l/vE+UfRbvgdrQNbkvboLa0DW5Lu+B2xPjHVGvKcDo1+aU2soutlNsdxIf6EeJnPqFvPRNCuEgybiSlFLf3vZ3/bfwfl3S6pFmOMSJhBC9veJn04nSPtEcvSVqCyWBiWNwwD0RX3dC4obyw/gVSi1KJC4yr/wVufhYjwzq1YVgj2uwb6o/MjZQ4CvnvOVfT/uJBfPL7Ib7YkMQ3f6TSOTqQawe1Y2TnSLalFbiTbw5bUwtwODVKQeeoIMadGUf/9mH0azuKh9duILnoJ64b8h8sxtrvH9daU1BmJ6OgjMMFrkR9uNCVrFPzSknOLWXdgRwKy6qXsIN8TSSG+ZMQ5kdiuCth7y77loXJMxkQPQQjPuzNSWJV6u/YnOWVr1OYsDgjwR6BvTyC0uJw7OUROK0RaHsIYCDQx0RCmJ/74X/MvCRrIU4Mqrn7Qq5N//799fr1671y7BPd3ry9XPr1pTw8+GGPXKk97qtxRPtH885573gguuoqYn1kyCOM7zze4/s/Hk+ufpJv9n3DsgnLKgfDKLU6+OaPVD5ec5A/k48Mk+hrNtA7IZT+7cPc1eFhx5RUV6euZtrP0zz298gvsZGUW0JybglJOaWuaW4pSTklJOeWYg9YjW/sfGz5vShLnciRXms1ylSAwZJFQGAevn65GH2ycBozKSMDJ0fGmA6zRHN5zDPkFfqTnOs6RnJuKUXlR/0Q8DERf1SijgutePjSJsCnxt7fmqrih0u5zUGbwOY5hhAnGqXUBq11/5rWScn4BNQxpCPxgfEsT17e5C//A/kH2J+/nwldJngouuo6hnQk2j+aVSmrTohk7HA6+OXQLwyPH15tVCo/i5GrBiRy1YBE/kzO44+kPHrEh9A9LqTejk4Gxw6md2Rv3t3yLpd1ugyzsWnt7iH+ZkL8Q+gRH3LMukX7F3H3sgX0DB/Elb0epqhME+pvISLAQnighXB35y5H95zm1E4ySjI4VHCIffn7eGn9S2y1zeTti9+u7JNba01Bqd39Q+BIgk52/zD4fV/2McnaYjQQE+JLXKgvcaF+xIf6ERviStTxoX7EhvpVu5CtuNxOZmE5mUXlZLmnmYXlZLmnmUVW1/LCcqwOV5W+2aiIDXHtOz7syDQh1PUjISbEt8Gd0VTQWlNud5JfaiO/1EaBe2p3aqKCfIgJ8SUy0OeEuSdeCEnGJyClFCMSRvDVnq8os5cd11CHFSqGZfR0e3EFpRTD4ofx84GfsTvtmAze/UhtythEdll2nVdR90oIpVdCaIP3qZTi5t43M33xdL7e+zVXdr7SA5Eea0XKCu5bcR99ovrw1ujXG3U7lUEZiAmIISYghoGxAzEajDyx+gk+2vZR5X3SSqk6fwho7WpzTs0rIy2/lNS8UlLyykjNKyUtv5Q1+3JILyjD4axemxbsayLYz0x2kZVS27HDkioFEQE+tAm0EBnkw2ltAogM8iEyyAeLyUBqXhkpeaWk5JawfHcmGYXl1e5zVwqig3yrJeowfzOFZfbKZFs14eaX2ikotVUm+9rPGbQJdCXm6GBfYoJ9j5p3XYxX9Wr/U53WmsJye2U/+GU2ByF+ZsL8XT8S5WLF4yfJ+AQ1ImEEn+34jHXp65o0wtKSpCV0DutMfGDTevGqy9C4oczfPZ8tWVvoE9Wn2Y7TEIsPLcbH6NOkc1aToXFD6dmmJ2//+TaDYgaRGJxY/4saYVPGJv6z5D90Cu3E6+c0LhHX5MrTr2Rlykpe3fgqA2MGckbEGfW+RilFqL+FUH8L3eKCa9zG7nCSUVhOWn6VRJ1XSkGZnYgAC22CfIgM9KmcRgb5EB5gwdiIauhyu4O0ygRdSrJ7mpJXwqakXL7fkobd3b4f7GsmxM/1CPZzXSjnmncvq7I+xM+M0aDIKCwjPb+c9IIyDueXkV5QRlJOCesO5JBXYjsmngCLkfBAV22EyaAwGQyYja6e6Cp6nqucVu2tzqgwGwyYTa5lFqMBc8XDpLC4X2M2GY5a7zpXpTYHxeUOSqz2ymlRuZ0Sq4PiiqnVTkm5e2p1YHc4CfI1E+Rrcj9qmjcTfNQyk0GRU2wjp7jcNdhMUfVBZ1zz5eQW1/0jx8dkIMzfQqi/O0EHmF2fKXfCrlge5GvCYjK4HkbX1Fx16p5vzOfmZCfJ+AQ1IGYAfia/yluSjkdeWR6bMzZzU8+bPBxddYNjB2NQBlamrGxUMnZqJ2vS1rBg9wIOFBzg+RHP0z6k/XHH4dROFh9czNC4oR4f4EEpxd0D7uZfi//Fld9cyb0D7+WyTpd55OKnHTk7+NfifxETEMNb575VOSZ2U+N9bMhjXLHwCu5dfi9zx871SMclJqOhsk25X7sm765GPiYj7dsE0L5NzX9Dh1NTbLUTaDEdZ1vzsbUCFcpsDg4XlJHuTtKu+XJyS6zYHM7KDm/sTleHODaHE6vd6UqEzirrHa4OcipeY3U4sTmc2Bz6mJqFhvI1GwiwmPD3MbqmFiOBPiaig3wrlxkNisIyO4VlNgrL7GQUlrE301VTUFhmx96IYwf5mCqbRuJDfekZH0x4gKsf/HB3s4mvyUh+qZXcEhu5JVbySmzkFrue55VY2ZleSF6JjbxS23G9b4OiMkH7mAz4mIz4W4z4+5gI9DHibzERUPncdU4CLCYCfEwEHLXe3+LqWdHf4lruazacUBcvSjI+QfkYfRgUM4jlKcuPa2SktKI0XtrwEg7tqLHXLU8K8QmhR5serEpdxS19bmlQbF/t+Yqv9nxFanEqwZZgDMrA5EWTee/89+gY0vG44tiStYXDJYf5d99/H9fr69Mnqg/zL5nPQyse4tFVj7Lk0BIeHfpok4ajPJB/gH/8/A8CLAHMHD2TCL8Ij8Ub6hvK08OfZtpP03hh3Qs8MuQRj+3bm4wGRXAzVR37mo20iwigXUTzjdZVkcQrkvPR81b37XH+FiMB7iTi7060TaG1pszmpLDMRkGVhO1K0k4iAly1GDUNNtNUFRfs5buTdmGZHZvDSbndWfmeK86Da5muXGatsr7M5nTVFFgdlJTbyS4qocR6pPagpmaS2ihFZXL2dU/9LCb8K+eNhPlbePLSHh47D3WRZHwCG5E4gqXJS9mXv4/TQk9r0GuyS7N5d8u7zN05F3B14dk9ontzhgm4RnF664+3yCvLI9Q39Jj1VoeVX5N+ZcHuBaxOXQ24StS397uds9ueTUphCjf9dBOTf5zMe+e9R6ewTo2OYfHBxZgMJkYmjmzq26lVTEAMM8+bySfbP+HVDa9yxcIreGzIY5zVtvE/eNKL05n6s2vM5JmjZxIbGOvpcBkcO5hJ3Scxe+tshsUP45y253j8GKJxjAaF0eBKAC1JKYWfO8lENb3ypdHHrmgqaBvRfAOvOJyaEmv1qnxX1b7dnbQdlFZO3ctsFcvslevT8m2U2hz4ePAHSX0adGuTUmoM8BpgBN7VWj971Po7gCmAHcgEbtRaH6xrn3JrU/3Si9MZ/cVo/tPvP/UO5VdoLeSDrR/w0baPKHOUcWmnS5nea3qzfMHXZHPGZq7/4XpeGPECYzqMqVy+M2cnC/Ys4Nt935Jfnk9sQCyXdrqUSzpdckw79v78/UxZNAWb08Y7573TqKEZtdZcMP8COoR0YMa5Mzz2vuqyJ3cP96+4nx05O7j89Mu5Z8A9Da4ezy7NZtKPk8gqzWLW+bMa1KZ7vGwOG9d+fy1pxWl8Oe5LovybPsKYEKLx6rq1qd60r5QyAm8AFwDdgKuVUt2O2mwT0F9r3Qv4Ani+aSELcJXCuoR1YVnyslq3KbWXMuuvWYz5cgxv//k2wxOG89UlX/H40MdbLBED9GjTgyBLECtTV1JgLWDujrlM/HYiV35zJfN2zmNw7GDePvdtfrj8B/555j9rvKCsQ0gHZo+ZjcVoYcpPU9iRs6PBx9+es52UohTOa3eeJ99WnTqFdeLTCz9lSs8pfLXnK65YeAWbMjbV+7pCayE3L76Z9OJ03jjnjWZNxABmo5nnRjxHuaOcB1Y8gFPXfZWxEKLlNaQMPhDYo7Xep7W2AnOAal1Paa2XaK1L3E9/BxI8G+apa0TCCDZnbCa/PL/acpvDxtwdc7lo/kW8suEVekX2Yu7Yubw48kU6hHRo8ThNBhODYwez6MAizp53Nk+teQqr08q9A+7l1/G/8uLIF139btczElXb4LbMHjMbP5MfNy26ia3ZWxt0/MUHF2NUxmZvHz+a2Wjm333/zezzZwMw6cdJvLbxNWyOY6/KBdePp1t+uYXdubt5edTL9I3u2yJxdgjpwL0D7mVN2ho+3PphixxTCNFwDUnG8UBSlefJ7mW1uQn4oaYVSqlpSqn1Sqn1mZmZDY/yFDYiYQQO7ahsZ3U4HXyz9xvGfTWOp9Y8RUJQAu+PeZ8Z586gW8TRFRYt6+KOFxNgDuCS0y5hzkVz+PJi12hHNbUh1yUxKJHZY2YTZAli6qKpbMncUuf2Wmt+Pvgz/WP6N/pYntI3ui9fjvuSSztdyrtb3uWa769hT+6eatvYHDbuWHoHmzI28czwZzx++1V9Lj/9cs5tey6vbXqNbdnbWvTYouG+2fsNT65+khJbSf0bi1aj3jZjpdSVwBit9RT38+uBQVrrYy6bVUpdB9wCjNRalx+9vippM24Yh9PBWfPOYlj8MEa3G83rm15nT94euoR14ba+tzE8fvgJdXm+J6UVpXHTTzeRW5bLjHNn1DqK1e7c3Vy+8HKPdVfZVEsOLeGx1Y9RZC3i333/zXXdrkNrzX3L7+PHAz/y6JBHm63jkPrkl+dz+cLL8Tf5M3fsXPzNzXcxjWicMnsZz6x9hvm75wPQs01P3jznTa/9wKxLUmESv6f9TqG1kIs6XER0QLS3Q6qV1WHF7rSfEJ/1utqMG5KMhwCPaa3Pdz+/H0Br/cxR250LvI4rEWfUF5Qk44a7f/n9fLvvWwDaBbfjljNv4bz251V2c9iapRenM+WnKWSWZDLj3Bk1VuvO2DyDGX/M4Nerfm3SbUaelFWaxeOrHmdp8lIGxQwiOiCahXsXNuhivOa2Nm0tU36awuWnX85jQx/zaizC5VDBIe5Yegc7c3cytedUukd0555l95AQlMDbo9/2yIAxTZFXlsea9DX8nvY7q1NXk1KUUrnOqIyc3fZsru56Nf2j+58whYOkwiQ+3/U5C3YvoNReym19buO6btd59XuzqcnYBOwCzgFSgHXANVrrrVW26YPrwq0xWuvdDQlKknHDbTi8gefWPseELhO4pNMlXu9ysqVllGQw5acplRc8DYgZUG39ZV9fRohPCO+Ped87AdZCa8383fN5bt1zlNpLuanHTdze73ZvhwXAKxteYdZfs3h11Kuc005ud/KmXw7+wkMrH8KgDDwz/JnKscHXpa/j1l9vJdgSzMzRM5vUIU5jlTvK2ZSxidWpq/k97Xe2Z29Howk0BzIgZgCDYwczJG4IJmVi3q55LNizgPzyfE4LOY0JXScw7rRxHu94pyEcTgcrUlYwZ+ccVqasxKAMnJV4FlanlWXJy+gb1Zenhj3l8R70GqpJydi9gwuBV3Hd2jRLa/20UuoJYL3WeqFSajHQE0hzv+SQ1npcXfuUZCwaI6s0iymLppBSlMLr57zO4NjBgKvTjIu/upj7Bt7HtWdc6+Uoa5ZUmMSWzC1c0OGCE6bUYHPYuO6H60gpSuHLi788oasZWyub08ZrG17jg20f0COiBy+NeumYYUi3ZW/j5sU3AzTrdSFO7WRHzo7Kku+mjE2UO8oxKRO9o3ozOHYwg2MH06NNjxoLA2X2Mn488CNzdsxha/ZW/E3+XHzaxUzsMvG4+gxorOzSbBbsWcDnOz8ntTiVSL9Irux8JVecfgXRAdForVm4dyHPrn0Wh3ZwZ787uarLVS3+77HJybg5SDIWjZVdms3Un6dyqOAQ/zvrfwyNH8q7W97ltY2v8fOVP3u9Ku9kcyD/AFd9exW92vRi5nkzT4lmjxPF4eLD3L3sbjZlbGJil4ncPeDuWsfKPpB/gGk/T6PAWsDrZ79+TM1QUxRZi3hnyzvM3z2fvPI8ADqFdmJI3BAGxw6mf3T/Rre1bsncwpydc/hx/49YnVYGxAxgYpeJnNX2LMwGz/WcprXmj8w/mLNzDj8d+Amb08bAmIFM6DKh1mOlF6fzyMpHWJ22msGxg3li6BMteguoJGPRauSW5TL1p6nsz9/PK2e9wv9t+j/MRjOfXPiJt0M7Kc3fPZ9HVz16QrRlnypWp67mvuX3UWov5fGhj3NBhwvqfU16cTrTf55OUmESL4x8gbPbnt2kGBxOB1/v/Zr/bfwf2WXZnNfuPEYljmJw7GAi/SObtO8KuWW5zN89n3k755FanEqUXxRXdr6SKztf2aRjlNhK+Hbft8zdOZddubsINAcy7rRxTOgygY6h9Xelq7Xm812f8+L6FzEqI/cMuIdLO13aIqVkScaiVckvz2fqT1PZnbcbu9POnf3uZFKPSd4O66SktebO3+5kyaElfHzRxy3SdWp9nNpJqb2UImsRxbZiCm2FFFuLKbK5nhfZijAqI/2i+3F62OknTYneqZ3M/HMmb25+k44hHXl51MsNSh4V8sry+Ocv/2Rb9jYeH/o4l3S6pP4X1WBd+jpeWPcC23O2c2bkmdw78F56tGm+/pcr2nE/2/kZK1NWYlIm+kb3xd/sj8VgwWw0YzYc9XAvsxgtmA1mTAYTZoOZPXl7WLh3IcW2YrqEdWFi14lc2OHC47pSOqkwiYdXPsyGwxsYmTCSR4c86rEfIrWRZCxanfzyfKb/PJ1tOdv47rLvSAiSfmaOV355PlcsvAJfky/zxs6r8YtNa02hrZCc0hyyy7LJKcshpzSHnDLX80JrIVprKv9zz1e8tuoyjcb1v8butFdLshVJt+K19Qn1CWVAzAAGxgxkYMxAOoR0OGHa5avKLcvl/hX3szJlJRd1vIhHBj9yXAmkxFbC7UtuZ3Xaau7qf1flWNUNkVyYzMsbXubng64mnTv63cGY9mNa9HwdKjjE3J1z2Xh4IzanrfrDUX3eru3HvN5sMHN++/OZ0GUCvSN7Nzl2p3byyfZPeG3ja/gYfXhw0IPNem2HJGPRKpXYSkguSqZzWGdvh3LSW5e+jpsW3cSw+GF0Cu1UmWSrJl+789gvR3AlxCBLEAZlQOH6ElNKUfmfqnmZQmFURgItgQSaAwkwB1TOB5oDCbAEVM4HWtzr3fMlthLWpa9jTdoa1qavJa3Yde1oG782DIgZwKCYQQyMGUhCUILXk/MfmX9w1293kV2azX0D72N85/FNisnqsHL/8vv56eBPTOk5hdv63Fbn/optxbzz5zt8uO1DTAYTN/a4kRu63+CRITWbk1M7sTvt1RK1n8mPQEugx4+1P38/D618iD8z/2R0u9E8NPghwn3DPX4cScZCiHrN+GMGMzbPwGwwE+EXQYRvBOF+4YT7HnlE+EW4pr6uaahvqEcvyjkeWmuSi5JZm7aWtemuR1ZpFgCxAbGuUnOsq+Tckhf5aa35dMenvLj+RaL9o3lp1EseawZwOB08teYpvtj1BVd2vpKHBj10TFezTu3k6z1f89rG18guy+bijhdzW9/b5ELHWtiddt7f+j5vbn6TIEsQjwx+xOO3/UkyFkI0SLmjHIvB4vXSZFNordlfsL8yOa9LX1d5pXBMQAxxAXFEB0QT4x/jmgbEVM6H+4Y3uA3aqZ1kl2aTWpxKWlEaqcWppBalklacVjktthUzMmEkT//taUJ8Qjz+Pl/f9DrvbHmH0e1G8+zwZyuvyK7om2B7znZ6R/bm3gH30jOyp0eP31rtzt3NgyseZHvO9iY1KdREkrEQ4pTl1E525+5mbfpatmVvI704nfTidA6XHMbmrD6gh9lgJso/ypWgA2KI9ncla3+TP2nFadUSbVpRGlantdrrgyxBxAXEERsYS1xAHN3bdGdsx7HNepHZB1s/4MX1LzIkdgj3DLiHGX/M4KeDPxHtH81/+v2HCztceFL/uPIGm9PGO3++w5q0Ncw6f1a9A9w0lCRjIYQ4itaanLIc0kvSOVx82JWkq8wfLjnM4ZLD1drKI3wjiA+Mr0y2R0+boz2zIb7a8xWPrXoMh3bga/Tlxh43MqnHpBO+XfhE53A6PJaIoe5kfGr1qyiEEG5KKVfbuF9ErW25Tu0kpyyHElsJ0QHR+Bh9WjjKhrm006WE+4azImUFN/a4UdqFPcSTibg+koyFEKIWBmVwDT5yEhQwRySMqOzXWpx8To675YUQQohWTJKxEEII4WWSjIUQQggvk2QshBBCeJkkYyGEEMLLJBkLIYQQXibJWAghhPAyScZCCCGEl0kyFkIIIbxMkrEQQgjhZZKMhRBCCC+TZCyEEEJ4mSRjIYQQwsskGQshhBBeJslYCCGE8DJJxkIIIYSXSTIWQgghvEySsRBCCOFlkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF4myVgIIYTwMknGQgghhJdJMhZCCCG8TJKxEEII4WWSjIUQQggvk2QshBBCeJkkYyGEEMLLJBkLIYQQXibJWAghhPAyScZCCCGEl0kyFkIIIbxMkrEQQgjhZZKMhRBCCC9rUDJWSo1RSu1USu1RSt1Xw3ofpdRc9/o1Sqn2Ho9UCCGEaKXqTcZKKSPwBnAB0A24WinV7ajNbgJytdadgFeA5zwdqBBCCNFaNaRkPBDYo7Xep7W2AnOAS47a5hLgA/f8F8A5SinluTCFEEKI1qshyTgeSKryPNm9rMZttNZ2IB+I8ESAQgghRGtnasmDKaWmAdPcT4uUUjs9uPs2QJYH99dayHmpmZyXmsl5qZmcl5rJealZbeelXW0vaEgyTgESqzxPcC+raZtkpZQJCAGyj96R1nomMLMBx2w0pdR6rXX/5tj3yUzOS83kvNRMzkvN5LzUTM5LzY7nvDSkmnodcLpSqoNSygJMBBYetc1C4Ab3/JXAr1pr3ZhAhBBCiFNVvSVjrbVdKXULsAgwArO01luVUk8A67XWC4H3gI+UUnuAHFwJWwghhBAN0KA2Y63198D3Ry17pMp8GTDes6E1WrNUf7cCcl5qJuelZnJeaibnpWZyXmrW6POipDZZCCGE8C7pDlMIIYTwslaRjOvrrvNUpZQ6oJTaopTarJRa7+14vEUpNUsplaGU+qvKsnCl1M9Kqd3uaZg3Y/SGWs7LY0qpFPdnZrNS6kJvxugNSqlEpdQSpdQ2pdRWpdS/3ctP6c9MHefllP7MKKV8lVJrlVJ/uM/L4+7lHdzdQ+9xdxdtqXM/J3s1tbu7zl3AaFwdkqwDrtZab/NqYCcApdQBoL/W+pS+D1ApNQIoAj7UWvdwL3seyNFaP+v+ARemtb7Xm3G2tFrOy2NAkdb6RW/G5k1KqVggVmu9USkVBGwALgUmcQp/Zuo4L1dxCn9m3L1NBmiti5RSZmAF8G/gDmC+1nqOUuot4A+t9Yza9tMaSsYN6a5TnMK01stwXeVfVdUuXD/A9aVySqnlvJzytNZpWuuN7vlCYDuuXgZP6c9MHefllKZditxPze6HBs7G1T00NODz0hqScUO66zxVaeAnpdQGd+9n4ohorXWaez4diPZmMCeYW5RSf7qrsU+pqtijuUeg6wOsQT4zlY46L3CKf2aUUkal1GYgA/gZ2AvkubuHhgbkpdaQjEXt/qa17otrxK1/uaslxVHcHdSc3O01njMDOA04E0gDXvJqNF6klAoEvgRu11oXVF13Kn9majgvp/xnRmvt0FqfiauHyoFA18buozUk44Z013lK0lqnuKcZwAJcHxLhctjdBlbRFpbh5XhOCFrrw+4vFifwDqfoZ8bd9vcl8InWer578Sn/manpvMhn5gitdR6wBBgChLq7h4YG5KXWkIwb0l3nKUcpFeC+yAKlVABwHvBX3a86pVTtwvUG4GsvxnLCqEg2bpdxCn5m3BfkvAds11q/XGXVKf2Zqe28nOqfGaVUpFIq1D3vh+ti4u24kvKV7s3q/byc9FdTA7gvpX+VI911Pu3diLxPKdURV2kYXD2tfXqqnhel1GfAKFwjqRwGHgW+AuYBbYGDwFVa61PqYqZazssoXNWNGjgA/KNKO+kpQSn1N2A5sAVwuhc/gKt99JT9zNRxXq7mFP7MKKV64bpAy4irgDtPa/2E+zt4DhAObAKu01qX17qf1pCMhRBCiJNZa6imFkIIIU5qkoyFEEIIL5NkLIQQQniZJGMhhBDCyyQZCyGEEF4myVgIIYTwMknGQgghhJdJMhZCCCG87P8B24ITxwYqp60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize = (8,5))\n",
    "plt.grid = True\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16067270934581757, 0.9435526728630066]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/anthony/Documents/GitHub/Research-Mapping-Uncanny-Valley/.venv/creepyvenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/anthony/Documents/GitHub/Research-Mapping-Uncanny-Valley/.venv/creepyvenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./models/self-tuned-model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/self-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "%rm -rf ./tf_tuner_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.IntInterval(8, 512))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_NUM_LAYERS = hp.HParam('n_layers', hp.IntInterval(2, 6))\n",
    "HP_LR = hp.HParam('learning rate', hp.RealInterval(0.0008, 0.001))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('tf_tuner_logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_NUM_LAYERS, HP_LR],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(units = hparams[HP_NUM_UNITS], input_shape = (769,), activation = 'relu'))\n",
    "    \n",
    "    for i in range(hparams[HP_NUM_LAYERS]):\n",
    "        model.add(Dense(units = hparams[HP_NUM_UNITS], activation = 'relu'))\n",
    "        model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "        \n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = keras.optimizers.Adam(lr=hparams[HP_LR]), # you can tune the learning rate here. Default lr = 0.01\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid)) # Run with 1 epoch to speed things up for demo purposes\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5900 - val_loss: 0.6789 - val_accuracy: 0.5848\n",
      "  1/178 [..............................] - ETA: 0s - loss: 0.6633 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_test_batch_end` time: 0.0013s). Check your callbacks.\n",
      "178/178 [==============================] - 0s 753us/step - loss: 0.6717 - accuracy: 0.6035\n",
      "--- Starting trial: run-1\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4946 - accuracy: 0.7515 - val_loss: 0.2818 - val_accuracy: 0.8973\n",
      "  1/178 [..............................] - ETA: 0s - loss: 0.3372 - accuracy: 0.9062WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_test_batch_end` time: 0.0069s). Check your callbacks.\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8970\n",
      "--- Starting trial: run-2\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5330 - accuracy: 0.7147 - val_loss: 0.3520 - val_accuracy: 0.8644\n",
      "178/178 [==============================] - 0s 807us/step - loss: 0.3392 - accuracy: 0.8751\n",
      "--- Starting trial: run-3\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4616 - accuracy: 0.7771 - val_loss: 0.2733 - val_accuracy: 0.8940\n",
      "178/178 [==============================] - 0s 552us/step - loss: 0.2735 - accuracy: 0.8989\n",
      "--- Starting trial: run-4\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4609 - accuracy: 0.7614 - val_loss: 0.2886 - val_accuracy: 0.8881\n",
      "178/178 [==============================] - 0s 776us/step - loss: 0.2875 - accuracy: 0.8929\n",
      "--- Starting trial: run-5\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.4955 - accuracy: 0.7372 - val_loss: 0.3540 - val_accuracy: 0.8385\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8506\n",
      "--- Starting trial: run-6\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6784 - accuracy: 0.5906 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 922us/step - loss: 0.6719 - accuracy: 0.6035\n",
      "--- Starting trial: run-7\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5069 - accuracy: 0.7444 - val_loss: 0.3386 - val_accuracy: 0.8744\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8763\n",
      "--- Starting trial: run-8\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.5280 - accuracy: 0.7125 - val_loss: 0.3365 - val_accuracy: 0.8688\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.3351 - accuracy: 0.8735\n",
      "--- Starting trial: run-9\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.5899 - accuracy: 0.6434 - val_loss: 0.4093 - val_accuracy: 0.8573\n",
      "178/178 [==============================] - 0s 799us/step - loss: 0.3954 - accuracy: 0.8665\n",
      "--- Starting trial: run-10\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.4748 - accuracy: 0.7690 - val_loss: 0.3066 - val_accuracy: 0.8938\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8963\n",
      "--- Starting trial: run-11\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6771 - accuracy: 0.5905 - val_loss: 0.6792 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 766us/step - loss: 0.6716 - accuracy: 0.6035\n",
      "--- Starting trial: run-12\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5526 - accuracy: 0.6973 - val_loss: 0.3699 - val_accuracy: 0.8404\n",
      "178/178 [==============================] - 0s 640us/step - loss: 0.3560 - accuracy: 0.8476\n",
      "--- Starting trial: run-13\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.4055 - val_accuracy: 0.8788\n",
      "178/178 [==============================] - 0s 879us/step - loss: 0.3998 - accuracy: 0.8823\n",
      "--- Starting trial: run-14\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.5368 - accuracy: 0.7118 - val_loss: 0.3044 - val_accuracy: 0.8827\n",
      "178/178 [==============================] - 0s 723us/step - loss: 0.2997 - accuracy: 0.8864\n",
      "--- Starting trial: run-15\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5760 - accuracy: 0.6717 - val_loss: 0.4288 - val_accuracy: 0.7962\n",
      "178/178 [==============================] - 0s 763us/step - loss: 0.4114 - accuracy: 0.8046\n",
      "--- Starting trial: run-16\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5665 - accuracy: 0.6760 - val_loss: 0.3974 - val_accuracy: 0.8822\n",
      "178/178 [==============================] - 0s 764us/step - loss: 0.3882 - accuracy: 0.8862\n",
      "--- Starting trial: run-17\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.5393 - accuracy: 0.7111 - val_loss: 0.4312 - val_accuracy: 0.7971\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8116\n",
      "--- Starting trial: run-18\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6777 - accuracy: 0.5916 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 883us/step - loss: 0.6719 - accuracy: 0.6035\n",
      "--- Starting trial: run-19\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6782 - accuracy: 0.5915 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 837us/step - loss: 0.6723 - accuracy: 0.6035\n",
      "--- Starting trial: run-20\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.7187 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0024s). Check your callbacks.\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6717 - accuracy: 0.5872 - val_loss: 0.6017 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 908us/step - loss: 0.5925 - accuracy: 0.6035\n",
      "--- Starting trial: run-21\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5811 - accuracy: 0.6570 - val_loss: 0.4146 - val_accuracy: 0.8353\n",
      "178/178 [==============================] - 0s 970us/step - loss: 0.3993 - accuracy: 0.8479\n",
      "--- Starting trial: run-22\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.6303 - val_loss: 0.4479 - val_accuracy: 0.8825\n",
      "  1/178 [..............................] - ETA: 0s - loss: 0.4392 - accuracy: 0.9062WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_test_batch_end` time: 0.0043s). Check your callbacks.\n",
      "178/178 [==============================] - 0s 929us/step - loss: 0.4420 - accuracy: 0.8869\n",
      "--- Starting trial: run-23\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6776 - accuracy: 0.5907 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 836us/step - loss: 0.6726 - accuracy: 0.6035\n",
      "--- Starting trial: run-24\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.5552 - accuracy: 0.7012 - val_loss: 0.3996 - val_accuracy: 0.8462\n",
      "178/178 [==============================] - 0s 876us/step - loss: 0.3830 - accuracy: 0.8561\n",
      "--- Starting trial: run-25\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5904 - accuracy: 0.6498 - val_loss: 0.4045 - val_accuracy: 0.8661\n",
      "178/178 [==============================] - 0s 964us/step - loss: 0.3910 - accuracy: 0.8762\n",
      "--- Starting trial: run-26\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.5935 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 851us/step - loss: 0.6719 - accuracy: 0.6035\n",
      "--- Starting trial: run-27\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.8431 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0014s vs `on_train_batch_end` time: 0.0040s). Check your callbacks.\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6802 - accuracy: 0.5915 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 827us/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-28\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6669 - accuracy: 0.6084 - val_loss: 0.6678 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 943us/step - loss: 0.6529 - accuracy: 0.6035\n",
      "--- Starting trial: run-29\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6793 - accuracy: 0.5885 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 871us/step - loss: 0.6727 - accuracy: 0.6035\n",
      "--- Starting trial: run-30\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6807 - accuracy: 0.5866 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 970us/step - loss: 0.6727 - accuracy: 0.6035\n",
      "--- Starting trial: run-31\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.8060 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0147s). Check your callbacks.\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6798 - accuracy: 0.5903 - val_loss: 0.6790 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 921us/step - loss: 0.6717 - accuracy: 0.6035\n",
      "--- Starting trial: run-32\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6791 - accuracy: 0.5908 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 805us/step - loss: 0.6726 - accuracy: 0.6035\n",
      "--- Starting trial: run-33\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5797 - accuracy: 0.6793 - val_loss: 0.3681 - val_accuracy: 0.8897\n",
      "178/178 [==============================] - 0s 998us/step - loss: 0.3606 - accuracy: 0.8931\n",
      "--- Starting trial: run-34\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.7382 - accuracy: 0.4062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.6778 - accuracy: 0.5919 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "  1/178 [..............................] - ETA: 0s - loss: 0.6640 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0027s). Check your callbacks.\n",
      "178/178 [==============================] - 0s 974us/step - loss: 0.6719 - accuracy: 0.6035\n",
      "--- Starting trial: run-35\n",
      "{'n_layers': 2, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6794 - accuracy: 0.5932 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 679us/step - loss: 0.6725 - accuracy: 0.6035\n",
      "--- Starting trial: run-36\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3499 - accuracy: 0.8379 - val_loss: 0.2227 - val_accuracy: 0.9151\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9203\n",
      "--- Starting trial: run-37\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3654 - accuracy: 0.8296 - val_loss: 0.2213 - val_accuracy: 0.9156\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9139\n",
      "--- Starting trial: run-38\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3527 - accuracy: 0.8388 - val_loss: 0.3736 - val_accuracy: 0.8360\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8389\n",
      "--- Starting trial: run-39\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3422 - accuracy: 0.8453 - val_loss: 0.3656 - val_accuracy: 0.8399\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8382\n",
      "--- Starting trial: run-40\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3513 - accuracy: 0.8329 - val_loss: 0.2128 - val_accuracy: 0.9176\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9169\n",
      "--- Starting trial: run-41\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3694 - accuracy: 0.8262 - val_loss: 0.2359 - val_accuracy: 0.9044\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9046\n",
      "--- Starting trial: run-42\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3644 - accuracy: 0.8315 - val_loss: 0.2234 - val_accuracy: 0.9160\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9160\n",
      "--- Starting trial: run-43\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3528 - accuracy: 0.8351 - val_loss: 0.2242 - val_accuracy: 0.9102\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2271 - accuracy: 0.9127\n",
      "--- Starting trial: run-44\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3801 - accuracy: 0.8195 - val_loss: 0.2567 - val_accuracy: 0.9024\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9044\n",
      "--- Starting trial: run-45\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3744 - accuracy: 0.8220 - val_loss: 0.2975 - val_accuracy: 0.8700\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8721\n",
      "--- Starting trial: run-46\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3789 - accuracy: 0.8189 - val_loss: 0.2497 - val_accuracy: 0.9100\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9111\n",
      "--- Starting trial: run-47\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3638 - accuracy: 0.8305 - val_loss: 0.7707 - val_accuracy: 0.7002\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7281 - accuracy: 0.7112\n",
      "--- Starting trial: run-48\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3761 - accuracy: 0.8233 - val_loss: 0.5532 - val_accuracy: 0.7426\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7328\n",
      "--- Starting trial: run-49\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3743 - accuracy: 0.8239 - val_loss: 0.2631 - val_accuracy: 0.8913\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8943\n",
      "--- Starting trial: run-50\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3662 - accuracy: 0.8317 - val_loss: 0.3434 - val_accuracy: 0.8503\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8557\n",
      "--- Starting trial: run-51\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3548 - accuracy: 0.8366 - val_loss: 0.2196 - val_accuracy: 0.9154\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9173\n",
      "--- Starting trial: run-52\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3633 - accuracy: 0.8305 - val_loss: 0.3148 - val_accuracy: 0.8682\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3214 - accuracy: 0.8645\n",
      "--- Starting trial: run-53\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3646 - accuracy: 0.8317 - val_loss: 0.2866 - val_accuracy: 0.8762\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.8813\n",
      "--- Starting trial: run-54\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3665 - accuracy: 0.8290 - val_loss: 0.2239 - val_accuracy: 0.9121\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9118\n",
      "--- Starting trial: run-55\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3592 - accuracy: 0.8322 - val_loss: 0.2083 - val_accuracy: 0.9214\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2086 - accuracy: 0.9199\n",
      "--- Starting trial: run-56\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3764 - accuracy: 0.8242 - val_loss: 0.2184 - val_accuracy: 0.9170\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9185\n",
      "--- Starting trial: run-57\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3626 - accuracy: 0.8333 - val_loss: 0.2503 - val_accuracy: 0.8978\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2459 - accuracy: 0.9092\n",
      "--- Starting trial: run-58\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3822 - accuracy: 0.8205 - val_loss: 0.2242 - val_accuracy: 0.9116\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9120\n",
      "--- Starting trial: run-59\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3786 - accuracy: 0.8208 - val_loss: 0.3139 - val_accuracy: 0.8769\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8843\n",
      "--- Starting trial: run-60\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3732 - accuracy: 0.8247 - val_loss: 0.3329 - val_accuracy: 0.8534\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8608\n",
      "--- Starting trial: run-61\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3883 - accuracy: 0.8181 - val_loss: 0.3491 - val_accuracy: 0.8459\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8476\n",
      "--- Starting trial: run-62\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3888 - accuracy: 0.8194 - val_loss: 0.3516 - val_accuracy: 0.8667\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8719\n",
      "--- Starting trial: run-63\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3648 - accuracy: 0.8303 - val_loss: 0.2860 - val_accuracy: 0.8815\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8836\n",
      "--- Starting trial: run-64\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3817 - accuracy: 0.8163 - val_loss: 0.2614 - val_accuracy: 0.9029\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2549 - accuracy: 0.9097\n",
      "--- Starting trial: run-65\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3746 - accuracy: 0.8251 - val_loss: 0.2354 - val_accuracy: 0.9107\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9155\n",
      "--- Starting trial: run-66\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3735 - accuracy: 0.8260 - val_loss: 0.2338 - val_accuracy: 0.9058\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2289 - accuracy: 0.9136\n",
      "--- Starting trial: run-67\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3792 - accuracy: 0.8225 - val_loss: 0.2496 - val_accuracy: 0.9082\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.9123\n",
      "--- Starting trial: run-68\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3733 - accuracy: 0.8251 - val_loss: 0.2237 - val_accuracy: 0.9109\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9136\n",
      "--- Starting trial: run-69\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3772 - accuracy: 0.8239 - val_loss: 0.4594 - val_accuracy: 0.8120\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8199\n",
      "--- Starting trial: run-70\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.3703 - accuracy: 0.8246 - val_loss: 0.2207 - val_accuracy: 0.9153\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9176\n",
      "--- Starting trial: run-71\n",
      "{'n_layers': 2, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 5s 6ms/step - loss: 0.4159 - accuracy: 0.7975 - val_loss: 0.2482 - val_accuracy: 0.9005\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.2466 - accuracy: 0.9039\n",
      "--- Starting trial: run-72\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.4517 - accuracy: 0.7671 - val_loss: 0.2672 - val_accuracy: 0.8954\n",
      "178/178 [==============================] - 0s 814us/step - loss: 0.2649 - accuracy: 0.8980\n",
      "--- Starting trial: run-73\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6778 - accuracy: 0.5926 - val_loss: 0.6791 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 694us/step - loss: 0.6716 - accuracy: 0.6035\n",
      "--- Starting trial: run-74\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4473 - accuracy: 0.7714 - val_loss: 0.3159 - val_accuracy: 0.8600\n",
      "178/178 [==============================] - 0s 873us/step - loss: 0.3055 - accuracy: 0.8719\n",
      "--- Starting trial: run-75\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5923 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 966us/step - loss: 0.6723 - accuracy: 0.6035\n",
      "--- Starting trial: run-76\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.7449 - val_loss: 0.2728 - val_accuracy: 0.8908\n",
      "178/178 [==============================] - 0s 967us/step - loss: 0.2675 - accuracy: 0.8936\n",
      "--- Starting trial: run-77\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.4966 - accuracy: 0.7563 - val_loss: 0.3485 - val_accuracy: 0.8867\n",
      "178/178 [==============================] - 0s 789us/step - loss: 0.3369 - accuracy: 0.8942\n",
      "--- Starting trial: run-78\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5612 - accuracy: 0.6868 - val_loss: 0.5843 - val_accuracy: 0.7631\n",
      "178/178 [==============================] - 0s 770us/step - loss: 0.5534 - accuracy: 0.7732\n",
      "--- Starting trial: run-79\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5868 - accuracy: 0.6535 - val_loss: 0.5083 - val_accuracy: 0.8198\n",
      "  1/178 [..............................] - ETA: 0s - loss: 0.4434 - accuracy: 0.7812WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_test_batch_end` time: 0.0018s). Check your callbacks.\n",
      "178/178 [==============================] - 0s 793us/step - loss: 0.4816 - accuracy: 0.8262\n",
      "--- Starting trial: run-80\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6773 - accuracy: 0.5922 - val_loss: 0.6790 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 714us/step - loss: 0.6717 - accuracy: 0.6035\n",
      "--- Starting trial: run-81\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.7080 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.6948 - val_loss: 0.3390 - val_accuracy: 0.8781\n",
      "178/178 [==============================] - 0s 661us/step - loss: 0.3260 - accuracy: 0.8839\n",
      "--- Starting trial: run-82\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.5758 - accuracy: 0.6767 - val_loss: 0.3860 - val_accuracy: 0.8763\n",
      "178/178 [==============================] - 0s 664us/step - loss: 0.3743 - accuracy: 0.8827\n",
      "--- Starting trial: run-83\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6182 - accuracy: 0.6313 - val_loss: 0.5334 - val_accuracy: 0.7224\n",
      "178/178 [==============================] - 0s 753us/step - loss: 0.5283 - accuracy: 0.7160\n",
      "--- Starting trial: run-84\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6783 - accuracy: 0.5923 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 652us/step - loss: 0.6720 - accuracy: 0.6035\n",
      "--- Starting trial: run-85\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6049 - accuracy: 0.6236 - val_loss: 0.4141 - val_accuracy: 0.8756\n",
      "178/178 [==============================] - 0s 555us/step - loss: 0.3995 - accuracy: 0.8783\n",
      "--- Starting trial: run-86\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6545 - accuracy: 0.6003 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 863us/step - loss: 0.6729 - accuracy: 0.6035\n",
      "--- Starting trial: run-87\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6793 - accuracy: 0.5915 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 809us/step - loss: 0.6724 - accuracy: 0.6035\n",
      "--- Starting trial: run-88\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6764 - accuracy: 0.5905 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6723 - accuracy: 0.6035\n",
      "--- Starting trial: run-89\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6779 - accuracy: 0.5929 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 932us/step - loss: 0.6723 - accuracy: 0.6035\n",
      "--- Starting trial: run-90\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6490 - accuracy: 0.5931 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 894us/step - loss: 0.6778 - accuracy: 0.6035\n",
      "--- Starting trial: run-91\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6796 - accuracy: 0.5914 - val_loss: 0.6787 - val_accuracy: 0.5848 - loss: 0.681\n",
      "178/178 [==============================] - 0s 696us/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-92\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6790 - accuracy: 0.5927 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 742us/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-93\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.6898 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0054s). Check your callbacks.\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6044 - accuracy: 0.6389 - val_loss: 0.4270 - val_accuracy: 0.8483\n",
      "178/178 [==============================] - 0s 948us/step - loss: 0.4164 - accuracy: 0.8465\n",
      "--- Starting trial: run-94\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6788 - accuracy: 0.5918 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 763us/step - loss: 0.6720 - accuracy: 0.6035\n",
      "--- Starting trial: run-95\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5922 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 799us/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-96\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5921 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 633us/step - loss: 0.6729 - accuracy: 0.6035\n",
      "--- Starting trial: run-97\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6789 - accuracy: 0.5917 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 786us/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-98\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6785 - accuracy: 0.5929 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 903us/step - loss: 0.6719 - accuracy: 0.6035\n",
      "--- Starting trial: run-99\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6799 - accuracy: 0.5923 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 637us/step - loss: 0.6724 - accuracy: 0.6035\n",
      "--- Starting trial: run-100\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6790 - accuracy: 0.5932 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 714us/step - loss: 0.6727 - accuracy: 0.6035\n",
      "--- Starting trial: run-101\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6783 - accuracy: 0.5931 - val_loss: 0.6789 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 981us/step - loss: 0.6718 - accuracy: 0.6035\n",
      "--- Starting trial: run-102\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 2s 2ms/step - loss: 0.6807 - accuracy: 0.5930 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 707us/step - loss: 0.6725 - accuracy: 0.6035\n",
      "--- Starting trial: run-103\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6813 - accuracy: 0.5918 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 817us/step - loss: 0.6731 - accuracy: 0.6035\n",
      "--- Starting trial: run-104\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.5926 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 938us/step - loss: 0.6720 - accuracy: 0.6035\n",
      "--- Starting trial: run-105\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6793 - accuracy: 0.5927 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 901us/step - loss: 0.6723 - accuracy: 0.6035\n",
      "--- Starting trial: run-106\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6797 - accuracy: 0.5926 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 645us/step - loss: 0.6721 - accuracy: 0.6035\n",
      "--- Starting trial: run-107\n",
      "{'n_layers': 6, 'num_units': 8, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 1s 2ms/step - loss: 0.6792 - accuracy: 0.5927 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 724us/step - loss: 0.6720 - accuracy: 0.6035\n",
      "--- Starting trial: run-108\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 10s 12ms/step - loss: 0.5901 - accuracy: 0.6577 - val_loss: 0.2876 - val_accuracy: 0.8841\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8887\n",
      "--- Starting trial: run-109\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 10s 12ms/step - loss: 0.6484 - accuracy: 0.6103 - val_loss: 0.9427 - val_accuracy: 0.6470\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.8893 - accuracy: 0.6652\n",
      "--- Starting trial: run-110\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 10s 13ms/step - loss: 0.5376 - accuracy: 0.7067 - val_loss: 0.3321 - val_accuracy: 0.8485\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3345 - accuracy: 0.8488: 0s - loss: 0.3369 - accuracy: 0.\n",
      "--- Starting trial: run-111\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 10s 13ms/step - loss: 0.3916 - accuracy: 0.8146 - val_loss: 0.7330 - val_accuracy: 0.6850\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.7029 - accuracy: 0.6984\n",
      "--- Starting trial: run-112\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.5769 - accuracy: 0.6677 - val_loss: 0.3161 - val_accuracy: 0.8660\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3160 - accuracy: 0.8665: 0s - loss: 0.3150 - ac\n",
      "--- Starting trial: run-113\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.0, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 10s 13ms/step - loss: 0.6448 - accuracy: 0.6124 - val_loss: 0.5378 - val_accuracy: 0.6687\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.6608\n",
      "--- Starting trial: run-114\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.5162 - accuracy: 0.7220 - val_loss: 0.3126 - val_accuracy: 0.8633\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3110 - accuracy: 0.8656\n",
      "--- Starting trial: run-115\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6316 - accuracy: 0.6364 - val_loss: 0.3806 - val_accuracy: 0.8402\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3634 - accuracy: 0.8492: 0s - loss: 0.3588 - accuracy: \n",
      "--- Starting trial: run-116\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6723 - accuracy: 0.5957 - val_loss: 0.6786 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.6035\n",
      "--- Starting trial: run-117\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.5759 - accuracy: 0.6731 - val_loss: 0.2846 - val_accuracy: 0.8857\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8890\n",
      "--- Starting trial: run-118\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6452 - accuracy: 0.6280 - val_loss: 0.4669 - val_accuracy: 0.6678\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4512 - accuracy: 0.6781: 0s - loss: 0.4473 - accuracy\n",
      "--- Starting trial: run-119\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.1, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6687 - accuracy: 0.5939 - val_loss: 0.6827 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.6035\n",
      "--- Starting trial: run-120\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6200 - accuracy: 0.6403 - val_loss: 0.3339 - val_accuracy: 0.8468\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3241 - accuracy: 0.8585\n",
      "--- Starting trial: run-121\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.5371 - accuracy: 0.7075 - val_loss: 0.4809 - val_accuracy: 0.7953\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4936 - accuracy: 0.7889\n",
      "--- Starting trial: run-122\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 10s 13ms/step - loss: 0.4858 - accuracy: 0.7418 - val_loss: 0.2627 - val_accuracy: 0.9084\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.2700 - accuracy: 0.9084\n",
      "--- Starting trial: run-123\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4127 - accuracy: 0.8059 - val_loss: 0.4097 - val_accuracy: 0.7981\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.7996\n",
      "--- Starting trial: run-124\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6667 - accuracy: 0.5946 - val_loss: 0.5777 - val_accuracy: 0.6937\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5672 - accuracy: 0.7105\n",
      "--- Starting trial: run-125\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.2, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6547 - accuracy: 0.6075 - val_loss: 0.4299 - val_accuracy: 0.8024\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4115 - accuracy: 0.8143\n",
      "--- Starting trial: run-126\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4272 - accuracy: 0.7914 - val_loss: 0.3746 - val_accuracy: 0.8450\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3717 - accuracy: 0.8469: 0s - loss: 0.3737 - accuracy: 0.84\n",
      "--- Starting trial: run-127\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4914 - accuracy: 0.7354 - val_loss: 0.3156 - val_accuracy: 0.8941\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.9005\n",
      "--- Starting trial: run-128\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6748 - accuracy: 0.5920 - val_loss: 0.6814 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-129\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4098 - accuracy: 0.8029 - val_loss: 0.2219 - val_accuracy: 0.9156\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.2237 - accuracy: 0.9176\n",
      "--- Starting trial: run-130\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4928 - accuracy: 0.7338 - val_loss: 0.3700 - val_accuracy: 0.8394\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8409\n",
      "--- Starting trial: run-131\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.3, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.5091 - accuracy: 0.7241 - val_loss: 0.2724 - val_accuracy: 0.8911\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8940\n",
      "--- Starting trial: run-132\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 14ms/step - loss: 0.4481 - accuracy: 0.7781 - val_loss: 0.2909 - val_accuracy: 0.8832\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.2842 - accuracy: 0.8887\n",
      "--- Starting trial: run-133\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6744 - accuracy: 0.5877 - val_loss: 0.6788 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.6035\n",
      "--- Starting trial: run-134\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 14ms/step - loss: 0.4577 - accuracy: 0.7681 - val_loss: 0.2952 - val_accuracy: 0.8829\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.2936 - accuracy: 0.8845\n",
      "--- Starting trial: run-135\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4300 - accuracy: 0.7883 - val_loss: 0.3082 - val_accuracy: 0.8832\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.3096 - accuracy: 0.8822\n",
      "--- Starting trial: run-136\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4594 - accuracy: 0.7652 - val_loss: 0.4278 - val_accuracy: 0.8288\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4260 - accuracy: 0.8278\n",
      "--- Starting trial: run-137\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.4, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6654 - accuracy: 0.5975 - val_loss: 0.6853 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.6035\n",
      "--- Starting trial: run-138\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6674 - accuracy: 0.5895 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6722 - accuracy: 0.6035\n",
      "--- Starting trial: run-139\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6804 - accuracy: 0.5887 - val_loss: 0.6787 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.6035\n",
      "--- Starting trial: run-140\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'adam', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6742 - accuracy: 0.5889 - val_loss: 0.6789 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6732 - accuracy: 0.6035\n",
      "--- Starting trial: run-141\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0008}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.4963 - accuracy: 0.7392 - val_loss: 0.2888 - val_accuracy: 0.8897\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.2834 - accuracy: 0.8926\n",
      "--- Starting trial: run-142\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.0009}\n",
      "829/829 [==============================] - 11s 13ms/step - loss: 0.6668 - accuracy: 0.5924 - val_loss: 0.6802 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.6035\n",
      "--- Starting trial: run-143\n",
      "{'n_layers': 6, 'num_units': 512, 'dropout': 0.5, 'optimizer': 'sgd', 'learning rate': 0.001}\n",
      "829/829 [==============================] - 13s 15ms/step - loss: 0.6822 - accuracy: 0.5908 - val_loss: 0.6793 - val_accuracy: 0.5848\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.6716 - accuracy: 0.6035\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for num_layers in tf.cast(tf.linspace(HP_NUM_LAYERS.domain.min_value, HP_NUM_LAYERS.domain.max_value, 2), tf.int64):\n",
    "    for num_units in tf.cast(tf.linspace(HP_NUM_UNITS.domain.min_value, HP_NUM_UNITS.domain.max_value, 2), tf.int64):\n",
    "        for dropout_rate in tf.linspace(HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value, 6):\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                for lr in tf.linspace(HP_LR.domain.min_value, HP_LR.domain.max_value, 3):\n",
    "                    hparams = {\n",
    "                        HP_NUM_LAYERS: num_layers.numpy(),\n",
    "                        HP_NUM_UNITS: num_units.numpy(),\n",
    "                        HP_DROPOUT: dropout_rate.numpy(),\n",
    "                        HP_OPTIMIZER: optimizer,\n",
    "                        HP_LR: lr.numpy()\n",
    "                    }\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print('--- Starting trial: %s' % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('tf_tuner_logs/hparam_tuning/' + run_name, hparams)\n",
    "                    session_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c058ea5aaca25c36\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c058ea5aaca25c36\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tf_tuner_logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world Test (r/shortscarystories and r/self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# model = keras.models.load_model('./models/self-tuned-model')\n",
    "model = keras.models.load_model('./models/keras-tuner-tuned-model')\n",
    "\n",
    "import spacy\n",
    "\n",
    "# spacy.prefer_gpu()\n",
    "# Must return True for GPU to work\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "text_creepy = \"\"\"\n",
    "Jeff the killers brother. Mc Nugget the killer. He ate so many chicken nuggets that he now craves chicken nuggets made of human flesh. So he kills and takes peoples flesh and makes them into chicken nuggies.\n",
    "\"\"\"\n",
    "\n",
    "text_non_creepy = \"\"\"\n",
    "Everything is incredibly hard for the moment.\n",
    "I got fired from my dream job because of reforms in the team. I'm currently suffering from a back injury which messes with my physical and mental health a lot. And now my long term partner wants so breakup with me. All this in a span of less than a month...\n",
    "I've been trying my best to keep my head up but when the nights grow longer, it's very difficult to stay positive and productive...\n",
    "I feel lost and don't know what to do now...\n",
    "\"\"\"\n",
    "\n",
    "# need nlp and model\n",
    "def sum_vec(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    comment_vec = sbert_model.encode(sentences)\n",
    "    return np.sum(comment_vec, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.28291309e-05],\n",
       "       [1.35453385e-11]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "creepy_vec = np.concatenate(([np.log(673+1+0.01)],sum_vec(text_creepy)))\n",
    "non_creepy_vec = np.concatenate(([np.log(325+1+0.01)],sum_vec(text_non_creepy)))\n",
    "vecs = np.array([creepy_vec, non_creepy_vec])\n",
    "vecs\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_vecs = scaler.fit_transform(vecs)\n",
    "\n",
    "model.predict(scaled_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fdb6d94fe20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_non_creepy)\n",
    "for idx, sents in enumerate(doc.sents):\n",
    "    pass\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./models/keras-tuner-tuned-model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 408)               314160    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                16360     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 344)               14104     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 344)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 328)               113160    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 460,425\n",
      "Trainable params: 460,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  1/829 [..............................] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000WARNING:tensorflow:From /Users/anthony/Documents/GitHub/Research-Mapping-Uncanny-Valley/.venv/creepyvenv/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0060s vs `on_train_batch_end` time: 0.0126s). Check your callbacks.\n",
      "829/829 [==============================] - 3s 4ms/step - loss: 0.1151 - accuracy: 0.9583 - val_loss: 0.1168 - val_accuracy: 0.9602\n",
      "Epoch 2/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1131 - accuracy: 0.9596 - val_loss: 0.1133 - val_accuracy: 0.9609\n",
      "Epoch 3/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1130 - accuracy: 0.9587 - val_loss: 0.1568 - val_accuracy: 0.9292\n",
      "Epoch 4/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1203 - accuracy: 0.9550 - val_loss: 0.1278 - val_accuracy: 0.9554\n",
      "Epoch 5/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1174 - accuracy: 0.9569 - val_loss: 0.1197 - val_accuracy: 0.9567\n",
      "Epoch 6/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1116 - accuracy: 0.9593 - val_loss: 0.1007 - val_accuracy: 0.9644\n",
      "Epoch 7/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1131 - accuracy: 0.9587 - val_loss: 0.1146 - val_accuracy: 0.9611\n",
      "Epoch 8/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1092 - accuracy: 0.9613 - val_loss: 0.1013 - val_accuracy: 0.9653\n",
      "Epoch 9/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1069 - accuracy: 0.9605 - val_loss: 0.1007 - val_accuracy: 0.9641\n",
      "Epoch 10/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1062 - accuracy: 0.9619 - val_loss: 0.1101 - val_accuracy: 0.9583\n",
      "Epoch 11/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1121 - accuracy: 0.9602 - val_loss: 0.1123 - val_accuracy: 0.9614\n",
      "Epoch 12/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1117 - accuracy: 0.9579 - val_loss: 0.1180 - val_accuracy: 0.9574\n",
      "Epoch 13/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1145 - accuracy: 0.9598 - val_loss: 0.1173 - val_accuracy: 0.9563\n",
      "Epoch 14/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1094 - accuracy: 0.9612 - val_loss: 0.1281 - val_accuracy: 0.9530\n",
      "Epoch 15/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1054 - accuracy: 0.9614 - val_loss: 0.0952 - val_accuracy: 0.9669\n",
      "Epoch 16/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1127 - accuracy: 0.9585 - val_loss: 0.1952 - val_accuracy: 0.9253\n",
      "Epoch 17/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1083 - accuracy: 0.9605 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 18/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1095 - accuracy: 0.9614 - val_loss: 0.0985 - val_accuracy: 0.9660\n",
      "Epoch 19/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1062 - accuracy: 0.9616 - val_loss: 0.1407 - val_accuracy: 0.9436\n",
      "Epoch 20/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1031 - accuracy: 0.9622 - val_loss: 0.1071 - val_accuracy: 0.9612\n",
      "Epoch 21/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9632 - val_loss: 0.2785 - val_accuracy: 0.9040\n",
      "Epoch 22/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1141 - accuracy: 0.9578 - val_loss: 0.1402 - val_accuracy: 0.9461\n",
      "Epoch 23/30\n",
      "829/829 [==============================] - 3s 3ms/step - loss: 0.1049 - accuracy: 0.9625 - val_loss: 0.1181 - val_accuracy: 0.9570\n",
      "Epoch 24/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1078 - accuracy: 0.9604 - val_loss: 0.1182 - val_accuracy: 0.9595\n",
      "Epoch 25/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1104 - accuracy: 0.9594 - val_loss: 0.1038 - val_accuracy: 0.9660\n",
      "Epoch 26/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1029 - accuracy: 0.9644 - val_loss: 0.1020 - val_accuracy: 0.9625\n",
      "Epoch 27/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1089 - accuracy: 0.9595 - val_loss: 0.1182 - val_accuracy: 0.9577\n",
      "Epoch 28/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1023 - accuracy: 0.9629 - val_loss: 0.1641 - val_accuracy: 0.9465\n",
      "Epoch 29/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1038 - accuracy: 0.9634 - val_loss: 0.1049 - val_accuracy: 0.9642\n",
      "Epoch 30/30\n",
      "829/829 [==============================] - 2s 3ms/step - loss: 0.1034 - accuracy: 0.9632 - val_loss: 0.1071 - val_accuracy: 0.9628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 52840), started 5:57:52 ago. (Use '!kill 52840' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c571593a5284850a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c571593a5284850a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "\n",
    "def get_run_log_dir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_log_dir()\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 30, \n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[tensorboard_cb])\n",
    "\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creepyvenv",
   "language": "python",
   "name": "creepyvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
