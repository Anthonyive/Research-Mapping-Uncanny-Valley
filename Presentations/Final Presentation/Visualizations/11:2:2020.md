This week I don't have a slide. I will simply show the code I wrote and the plots I drew. 

# Visualizing vectors

So the first set of plots is the visualization of our vectors. Our vectors are encoded by pretrained sbert model. Each has shape of 768 plus one score bias dimension. Also, they are divided by the sentence count.

Let print first 10 stories from creepy subreddit. We can see that vectors are bounded above and below aorund 1 unit. 

Compared to non creepy subreddit. Non creepy subreddit's vectors have little bit more range than ones from creepy subreddit. We can see that vectors are bounded above and below up to 2 units. 

This gives us a sense of how vectors are differ across subreddits.



# Distribution of scores and log scores

I also plotted distributions of scores and log scores. Just seeing the raw score, we can see that most of posts don't get many upvotes. Majority of them only has 1 upvotes. Very few posts has more than 2000 upvotes. Same for creepy and non creepy subreddits.



# Sentence Count

I also plotted how number of sentences distributed across two subreddits. 



---

These plots are not particularly interesting. However, this week I found a keras layer called embeddings. A [Keras Embedding Layer](https://keras.io/layers/embeddings/) can be used to train an embedding for each word in a volcabulary. I didn't use it that way, instead, I used it to convert our input shape to vectors of length 16. Then I used tensorboard to visualize the results.

Using the **TensorBoard Embedding Projector**, I can graphically represent high dimensional embeddings. I will show you...like this...

All the nodes represent story vectors reduced to 3D space. We can also color it to show which are creepy and which are not.

It uses PCA to reduce the dimension of data. Three principal components give us 38% variances. Not high, but good enough for visualizing our data.

Besides PCA, it can also use t-SNE to visualize the data. Let's pause it for a moment. If you guys are intereted, I can send you a link about how it works. Clearly I don't know how it works. This is my first time using it, but obviously it is a popular method for exploring high-dimensional data for those who understand it.

We have several parameters to play with. 

I think our data is really large, so I drag this all the way up to 100.

