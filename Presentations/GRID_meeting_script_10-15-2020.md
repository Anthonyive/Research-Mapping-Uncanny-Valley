Hi everyone, my name is Anthony Zhang. I am working on Professor Keith's project Uncanny Valley.

In the first week, Professor Keith gave us a general idea of what this project is all about. The data are regular and creepy subreddits from reddit. First thing I did was data cleaning and prepared it ready for spaCy since there are lots of posts that are either removed or deleted. Then I tried to remove stop words from each story. I created a word list containing several words that may lead to a creepy story. Then I compared each token in a text and find the similarities between each text and each word in my word list. I will use the maximum similarity to represent the "creepiness level" of a story. After our second meeting, there are lots of things I need to correct. Firstly, removing stop words is turned out to be wrong since we were not focusing on rough meaning of a story but rather the sementics. Secondly, the method in general, is very primitive and also time consuming. Considering I am new to NLP, I think it was a good start.

In the second week, Professor Keith suggested me thinking about word embeddings rather than primitively finding max similarities and he suggested me to explore how similar a story is to "human". Firstly I stop using stop words and using lemmatization instead. It takes into consideration the context of the word to determine what is the intended meaning in the text. I tried word2vec first using min_count = 1 and size 100. However, word2vec does not help me find similarity between a token in the story and an out of vacabulary word. So after some research, I found fasttext might be the model I need since it can compare with words it has never seen before. However, the result was far less than satisfactory. In the third week, I was preparing for midterms, and I was thinking, maybe this is not a good approach since my idea was always training a word embedding model per story and it didn't make sense for me to combine all stories to make a larger model. Therefore, the vocabularies and things I can do were quite limited. 

So this week, I am learning a paper Professor Keith sent to us. After a little bit of quick coding, I found it's a really great model to enable sentence embeddings. My current idea is adding up all sentence vectors to find a story vector and find a linear combination of comment vectors to get the “evaluations” of this story vector. I will do these steps for r/NoSleep and r/Confessions, one is creepy subreddit while another is not, to make classifications. This time I would like to combine comments in each story and hopefully I can get a better result.

